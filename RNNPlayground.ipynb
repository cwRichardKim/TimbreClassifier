{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Reccurent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "import numpy, scipy, matplotlib.pyplot as plt, pandas, librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# slicelength = 50\n",
    "# feature_power = 2 #TODO play with this\n",
    "# data_proportion = .01\n",
    "\n",
    "# classical_names = [f for f in os.listdir(\"./classical_short_%i/\" % slicelength)]\n",
    "# country_names = [f for f in os.listdir(\"./country_short_%i/\" % slicelength)]\n",
    "\n",
    "# num_datapoints = int(min(len(classical_names), len(country_names)) * data_proportion)\n",
    "# print \"num datapoints: \" + str(num_datapoints)\n",
    "\n",
    "# random.shuffle(classical_names)\n",
    "# random.shuffle(country_names)\n",
    "# classical_names = classical_names[0:num_datapoints]\n",
    "# country_names = country_names[0:num_datapoints]\n",
    "\n",
    "# def stft_array(names, kind):\n",
    "#     update_per = len(names)/10\n",
    "#     print \"update per %i\" % update_per\n",
    "#     array = []\n",
    "#     for i, x in enumerate(names):\n",
    "#         song_path = '%s_short_%i/%s' % (kind, slicelength, x)\n",
    "#         if i%update_per == 0:\n",
    "#             sys.stdout.write(str(i/update_per) + \" \")\n",
    "#             print \"adding \" + song_path + \" to \" + kind\n",
    "#         csong_stft = librosa.stft(librosa.load(song_path)[0])\n",
    "#         csong_log_stft = librosa.logamplitude(np.abs(csong_stft)**feature_power,ref_power=np.max)\n",
    "#         csong_log_stft = np.array([[abs(item)/80.0 for item in sublist] for sublist in csong_log_stft]) #TODO: consider removing abs / 80\n",
    "#         array.append(csong_log_stft.T)\n",
    "#     return array\n",
    "\n",
    "# #shape is 4 by 1025\n",
    "# classical_train = stft_array(classical_names, 'classical')\n",
    "# country_train = stft_array(country_names, 'country')\n",
    "\n",
    "# test_classical = []\n",
    "# test_country = []\n",
    "\n",
    "# print \"number of classical slices: %i\" % len(classical_train)\n",
    "\n",
    "# while len(test_classical) < len(classical_train) / 10:\n",
    "#     r = randint(0, len(classical_train) - 1)\n",
    "#     test_classical.append(classical_train[r])\n",
    "#     del classical_train[r]\n",
    "    \n",
    "# print \"number of classical training slices: %i\" % len(classical_train)\n",
    "# print \"number of classical testing slices: %i\" % len(test_classical)\n",
    "\n",
    "# print \"number of country slices: %i\" % len(country_train)\n",
    "# while len(test_country) < len(country_train) / 10:\n",
    "#     r = randint(0, len(country_train) - 1)\n",
    "#     test_country.append(country_train[r])\n",
    "#     del country_train[r]\n",
    "    \n",
    "# print \"number of country training slices: %i\" % len(country_train)\n",
    "# print \"number of country testing slices: %i\" % len(test_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing_with_mnist = False\n",
    "\n",
    "# if testing_with_mnist:\n",
    "#     # testing to see if mnist will work here\n",
    "#     from tensorflow.examples.tutorials.mnist import input_data\n",
    "#     mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "#     training = mnist.train.images\n",
    "#     testing = mnist.test.images\n",
    "#     training_validation_one_hot = mnist.train.labels\n",
    "#     testing_validation_one_hot = mnist.test.labels\n",
    "# else:\n",
    "#     training = (numpy.array(classical_train + country_train)).astype(numpy.float32)\n",
    "#     testing = (numpy.array(test_classical + test_country)).astype(numpy.float32)\n",
    "#     classical_one_hot = numpy.array([0,1])\n",
    "#     country_one_hot = numpy.array([1,0])\n",
    "#     training_validation_one_hot = (numpy.array([classical_one_hot for x in classical_train] + [country_one_hot for x in country_train])).astype(numpy.float32)\n",
    "#     testing_validation_one_hot = (numpy.array([classical_one_hot for x in test_classical] + [country_one_hot for x in test_country])).astype(numpy.float32)\n",
    "\n",
    "# def type_check(array_types, num_types):\n",
    "#     error_flag = False\n",
    "#     for x in array_types:\n",
    "#         if type(x) is numpy.ndarray:\n",
    "#             print \"ARRAY TYPE ERROR\"\n",
    "#             error_flag = True\n",
    "#     for x in num_types:\n",
    "#         if type(x) is numpy.float32:\n",
    "#             print \"NUM TYPE ERROR\"\n",
    "#             error_flag = True \n",
    "#     if not error_flag:\n",
    "#         print \"All types check out\"\n",
    "        \n",
    "# array_types = [type(training), type(training[0]), type(training_validation_one_hot), type(training_validation_one_hot[0]),\n",
    "#               type(testing), type(testing[0]), type(testing_validation_one_hot), type(testing_validation_one_hot[0])]\n",
    "# num_types = [type(training[0][0]), type(training_validation_one_hot[0][0]), type(testing[0][0]), type(testing_validation_one_hot[0][0])]\n",
    "# type_check(array_types, num_types)      \n",
    "\n",
    "# if len(training)==len(training_validation_one_hot) and len(testing) == len(testing_validation_one_hot) and len(training[0])==len(testing[0]):\n",
    "#     print \"training: \" + str(len(training))\n",
    "#     print \"testing: \" + str(len(testing))\n",
    "#     print \"num features: \" + str(len(training[0]))\n",
    "# else:\n",
    "#     print \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def shuffle_array(xs, ys):\n",
    "#     zipped = zip(xs,ys)\n",
    "#     random.shuffle(zipped)\n",
    "#     return zip(*zipped)\n",
    "# # print training_validation_one_hot[0:10]\n",
    "# type_check(array_types, num_types)\n",
    "\n",
    "# training, training_validation_one_hot = shuffle_array(training, training_validation_one_hot)\n",
    "# testing, testing_validation_one_hot = shuffle_array(testing, testing_validation_one_hot)\n",
    "\n",
    "# # print training_validation_one_hot[0:10]\n",
    "# type_check(array_types, num_types)\n",
    "\n",
    "# # print training[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variable_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10 Iter 1640, Minibatch Loss= 0.582995, Training Accuracy= 0.68902\n",
      "Iter 3280, Minibatch Loss= 0.540747, Training Accuracy= 0.73171\n",
      "Iter 4920, Minibatch Loss= 0.604354, Training Accuracy= 0.67073\n",
      "Iter 6560, Minibatch Loss= 0.582378, Training Accuracy= 0.69512\n",
      "Iter 8200, Minibatch Loss= 0.634488, Training Accuracy= 0.67073\n",
      "Iter 9840, Minibatch Loss= 0.593079, Training Accuracy= 0.72561\n",
      "Iter 11480, Minibatch Loss= 0.555769, Training Accuracy= 0.71341\n",
      "Iter 13120, Minibatch Loss= 0.629985, Training Accuracy= 0.66463\n",
      "Iter 14760, Minibatch Loss= 0.567251, Training Accuracy= 0.67073\n",
      "Iter 16400, Minibatch Loss= 0.534926, Training Accuracy= 0.72561\n",
      "Iter 18040, Minibatch Loss= 0.537385, Training Accuracy= 0.70732\n",
      "Iter 19680, Minibatch Loss= 0.544024, Training Accuracy= 0.68902\n",
      "Iter 21320, Minibatch Loss= 0.570680, Training Accuracy= 0.66463\n",
      "Iter 22960, Minibatch Loss= 0.568547, Training Accuracy= 0.75000\n",
      "Iter 24600, Minibatch Loss= 0.550433, Training Accuracy= 0.69512\n",
      "Iter 26240, Minibatch Loss= 0.512574, Training Accuracy= 0.75000\n",
      "Iter 27880, Minibatch Loss= 0.547764, Training Accuracy= 0.68902\n",
      "Iter 29520, Minibatch Loss= 0.494454, Training Accuracy= 0.75610\n",
      "Iter 31160, Minibatch Loss= 0.478029, Training Accuracy= 0.81098\n",
      "Iter 32800, Minibatch Loss= 0.615778, Training Accuracy= 0.65854\n",
      "Iter 34440, Minibatch Loss= 0.506839, Training Accuracy= 0.76220\n",
      "Iter 36080, Minibatch Loss= 0.550510, Training Accuracy= 0.68293\n",
      "Iter 37720, Minibatch Loss= 0.509171, Training Accuracy= 0.75000\n",
      "Iter 39360, Minibatch Loss= 0.621305, Training Accuracy= 0.64024\n",
      "Iter 41000, Minibatch Loss= 0.628347, Training Accuracy= 0.66463\n",
      "Iter 42640, Minibatch Loss= 0.496681, Training Accuracy= 0.76829\n",
      "Iter 44280, Minibatch Loss= 0.540861, Training Accuracy= 0.73171\n",
      "Iter 45920, Minibatch Loss= 0.558715, Training Accuracy= 0.71341\n",
      "Iter 47560, Minibatch Loss= 0.466866, Training Accuracy= 0.77439\n",
      "Iter 49200, Minibatch Loss= 0.594702, Training Accuracy= 0.67683\n",
      "Iter 50840, Minibatch Loss= 0.498904, Training Accuracy= 0.75000\n",
      "Iter 52480, Minibatch Loss= 0.564418, Training Accuracy= 0.71951\n",
      "Iter 54120, Minibatch Loss= 0.582557, Training Accuracy= 0.73780\n",
      "Iter 55760, Minibatch Loss= 0.494098, Training Accuracy= 0.76829\n",
      "Iter 57400, Minibatch Loss= 0.477097, Training Accuracy= 0.76829\n",
      "Iter 59040, Minibatch Loss= 0.477571, Training Accuracy= 0.74390\n",
      "Iter 60680, Minibatch Loss= 0.516400, Training Accuracy= 0.73171\n",
      "Iter 62320, Minibatch Loss= 0.509297, Training Accuracy= 0.74390\n",
      "Iter 63960, Minibatch Loss= 0.527430, Training Accuracy= 0.71341\n",
      "Iter 65600, Minibatch Loss= 0.458945, Training Accuracy= 0.79878\n",
      "Iter 67240, Minibatch Loss= 0.555958, Training Accuracy= 0.70732\n",
      "Iter 68880, Minibatch Loss= 0.488485, Training Accuracy= 0.77439\n",
      "Iter 70520, Minibatch Loss= 0.471751, Training Accuracy= 0.77439\n",
      "Iter 72160, Minibatch Loss= 0.497656, Training Accuracy= 0.79268\n",
      "Iter 73800, Minibatch Loss= 0.548944, Training Accuracy= 0.75610\n",
      "Iter 75440, Minibatch Loss= 0.520553, Training Accuracy= 0.74390\n",
      "Iter 77080, Minibatch Loss= 0.519463, Training Accuracy= 0.74390\n",
      "Iter 78720, Minibatch Loss= 0.492889, Training Accuracy= 0.80488\n",
      "Iter 80360, Minibatch Loss= 0.471520, Training Accuracy= 0.79268\n",
      "Iter 82000, Minibatch Loss= 0.514657, Training Accuracy= 0.71341\n",
      "Iter 83640, Minibatch Loss= 0.519750, Training Accuracy= 0.75610\n",
      "Iter 85280, Minibatch Loss= 0.525230, Training Accuracy= 0.72561\n",
      "Iter 86920, Minibatch Loss= 0.511671, Training Accuracy= 0.77439\n",
      "Iter 88560, Minibatch Loss= 0.498469, Training Accuracy= 0.79268\n",
      "Iter 90200, Minibatch Loss= 0.500594, Training Accuracy= 0.74390\n",
      "Iter 91840, Minibatch Loss= 0.512327, Training Accuracy= 0.71341\n",
      "Iter 93480, Minibatch Loss= 0.571969, Training Accuracy= 0.68902\n",
      "Iter 95120, Minibatch Loss= 0.476916, Training Accuracy= 0.81098\n",
      "Iter 96760, Minibatch Loss= 0.485905, Training Accuracy= 0.76829\n",
      "Iter 98400, Minibatch Loss= 0.499939, Training Accuracy= 0.74390\n",
      "Iter 100040, Minibatch Loss= 0.398205, Training Accuracy= 0.85976\n",
      "Iter 101680, Minibatch Loss= 0.455436, Training Accuracy= 0.75610\n",
      "Iter 103320, Minibatch Loss= 0.506800, Training Accuracy= 0.74390\n",
      "Iter 104960, Minibatch Loss= 0.444471, Training Accuracy= 0.82317\n",
      "Iter 106600, Minibatch Loss= 0.488773, Training Accuracy= 0.74390\n",
      "Iter 108240, Minibatch Loss= 0.511111, Training Accuracy= 0.78049\n",
      "Iter 109880, Minibatch Loss= 0.529973, Training Accuracy= 0.74390\n",
      "Iter 111520, Minibatch Loss= 0.589850, Training Accuracy= 0.68902\n",
      "Iter 113160, Minibatch Loss= 0.468236, Training Accuracy= 0.79878\n",
      "Iter 114800, Minibatch Loss= 0.461833, Training Accuracy= 0.77439\n",
      "Iter 116440, Minibatch Loss= 0.501989, Training Accuracy= 0.75000\n",
      "Iter 118080, Minibatch Loss= 0.541374, Training Accuracy= 0.73780\n",
      "Iter 119720, Minibatch Loss= 0.462767, Training Accuracy= 0.77439\n",
      "Iter 121360, Minibatch Loss= 0.499459, Training Accuracy= 0.75000\n",
      "Iter 123000, Minibatch Loss= 0.464409, Training Accuracy= 0.75610\n",
      "Iter 124640, Minibatch Loss= 0.411754, Training Accuracy= 0.83537\n",
      "Iter 126280, Minibatch Loss= 0.442065, Training Accuracy= 0.82927\n",
      "Iter 127920, Minibatch Loss= 0.493261, Training Accuracy= 0.70122\n",
      "Iter 129560, Minibatch Loss= 0.449162, Training Accuracy= 0.79878\n",
      "Iter 131200, Minibatch Loss= 0.424493, Training Accuracy= 0.84756\n",
      "Iter 132840, Minibatch Loss= 0.475985, Training Accuracy= 0.75610\n",
      "Iter 134480, Minibatch Loss= 0.438947, Training Accuracy= 0.83537\n",
      "Iter 136120, Minibatch Loss= 0.523396, Training Accuracy= 0.76829\n",
      "Iter 137760, Minibatch Loss= 0.442362, Training Accuracy= 0.79878\n",
      "Iter 139400, Minibatch Loss= 0.524306, Training Accuracy= 0.75000\n",
      "Iter 141040, Minibatch Loss= 0.447007, Training Accuracy= 0.78049\n",
      "Iter 142680, Minibatch Loss= 0.478745, Training Accuracy= 0.79268\n",
      "Iter 144320, Minibatch Loss= 0.484807, Training Accuracy= 0.78659\n",
      "Iter 145960, Minibatch Loss= 0.478202, Training Accuracy= 0.80488\n",
      "Iter 147600, Minibatch Loss= 0.473885, Training Accuracy= 0.78049\n",
      "Iter 149240, Minibatch Loss= 0.506886, Training Accuracy= 0.73780\n",
      "Iter 150880, Minibatch Loss= 0.460416, Training Accuracy= 0.81098\n",
      "Iter 152520, Minibatch Loss= 0.516166, Training Accuracy= 0.72561\n",
      "Iter 154160, Minibatch Loss= 0.438397, Training Accuracy= 0.79878\n",
      "Iter 155800, Minibatch Loss= 0.495108, Training Accuracy= 0.76220\n",
      "Iter 157440, Minibatch Loss= 0.469314, Training Accuracy= 0.76829\n",
      "Iter 159080, Minibatch Loss= 0.525084, Training Accuracy= 0.73171\n",
      "Iter 160720, Minibatch Loss= 0.548750, Training Accuracy= 0.73171\n",
      "Iter 162360, Minibatch Loss= 0.497501, Training Accuracy= 0.74390\n",
      "Iter 164000, Minibatch Loss= 0.475480, Training Accuracy= 0.81098\n",
      "Iter 165640, Minibatch Loss= 0.534201, Training Accuracy= 0.73171\n",
      "Iter 167280, Minibatch Loss= 0.572205, Training Accuracy= 0.70122\n",
      "Iter 168920, Minibatch Loss= 0.552323, Training Accuracy= 0.70732\n",
      "Iter 170560, Minibatch Loss= 0.453514, Training Accuracy= 0.79268\n",
      "Iter 172200, Minibatch Loss= 0.460034, Training Accuracy= 0.76829\n",
      "Iter 173840, Minibatch Loss= 0.514206, Training Accuracy= 0.75610\n",
      "Iter 175480, Minibatch Loss= 0.466163, Training Accuracy= 0.78049\n",
      "Iter 177120, Minibatch Loss= 0.442202, Training Accuracy= 0.80488\n",
      "Iter 178760, Minibatch Loss= 0.476943, Training Accuracy= 0.78049\n",
      "Iter 180400, Minibatch Loss= 0.479131, Training Accuracy= 0.74390\n",
      "Iter 182040, Minibatch Loss= 0.480340, Training Accuracy= 0.74390\n",
      "Iter 183680, Minibatch Loss= 0.452888, Training Accuracy= 0.78049\n",
      "Iter 185320, Minibatch Loss= 0.408173, Training Accuracy= 0.79878\n",
      "Iter 186960, Minibatch Loss= 0.513144, Training Accuracy= 0.75000\n",
      "Iter 188600, Minibatch Loss= 0.479133, Training Accuracy= 0.77439\n",
      "Iter 190240, Minibatch Loss= 0.442236, Training Accuracy= 0.80488\n",
      "Iter 191880, Minibatch Loss= 0.439613, Training Accuracy= 0.79878\n",
      "Iter 193520, Minibatch Loss= 0.414432, Training Accuracy= 0.85366\n",
      "Iter 195160, Minibatch Loss= 0.502237, Training Accuracy= 0.76220\n",
      "Iter 196800, Minibatch Loss= 0.518879, Training Accuracy= 0.73171\n",
      "Iter 198440, Minibatch Loss= 0.451074, Training Accuracy= 0.81707\n",
      "Iter 200080, Minibatch Loss= 0.543089, Training Accuracy= 0.71951\n",
      "Iter 201720, Minibatch Loss= 0.470782, Training Accuracy= 0.79268\n",
      "Iter 203360, Minibatch Loss= 0.445686, Training Accuracy= 0.81707\n",
      "Iter 205000, Minibatch Loss= 0.471427, Training Accuracy= 0.82317\n",
      "Iter 206640, Minibatch Loss= 0.535893, Training Accuracy= 0.71341\n",
      "Iter 208280, Minibatch Loss= 0.493519, Training Accuracy= 0.75610\n",
      "Iter 209920, Minibatch Loss= 0.383081, Training Accuracy= 0.85366\n",
      "Iter 211560, Minibatch Loss= 0.487371, Training Accuracy= 0.76829\n",
      "Iter 213200, Minibatch Loss= 0.499997, Training Accuracy= 0.77439\n",
      "Iter 214840, Minibatch Loss= 0.519693, Training Accuracy= 0.73780\n",
      "Iter 216480, Minibatch Loss= 0.543465, Training Accuracy= 0.68902\n",
      "Iter 218120, Minibatch Loss= 0.442043, Training Accuracy= 0.81098\n",
      "Iter 219760, Minibatch Loss= 0.473483, Training Accuracy= 0.79268\n",
      "Iter 221400, Minibatch Loss= 0.448080, Training Accuracy= 0.80488\n",
      "Iter 223040, Minibatch Loss= 0.510822, Training Accuracy= 0.75610\n",
      "Iter 224680, Minibatch Loss= 0.473785, Training Accuracy= 0.79268\n",
      "Iter 226320, Minibatch Loss= 0.395598, Training Accuracy= 0.81707\n",
      "Iter 227960, Minibatch Loss= 0.500662, Training Accuracy= 0.74390\n",
      "Iter 229600, Minibatch Loss= 0.523220, Training Accuracy= 0.75000\n",
      "Iter 231240, Minibatch Loss= 0.421260, Training Accuracy= 0.81098\n",
      "Iter 232880, Minibatch Loss= 0.388683, Training Accuracy= 0.84756\n",
      "Iter 234520, Minibatch Loss= 0.520795, Training Accuracy= 0.73780\n",
      "Iter 236160, Minibatch Loss= 0.458879, Training Accuracy= 0.78049\n",
      "Iter 237800, Minibatch Loss= 0.450485, Training Accuracy= 0.79268\n",
      "Iter 239440, Minibatch Loss= 0.399865, Training Accuracy= 0.81707\n",
      "Iter 241080, Minibatch Loss= 0.443521, Training Accuracy= 0.80488\n",
      "Iter 242720, Minibatch Loss= 0.435660, Training Accuracy= 0.79268\n",
      "Iter 244360, Minibatch Loss= 0.426910, Training Accuracy= 0.82317\n",
      "Iter 246000, Minibatch Loss= 0.440371, Training Accuracy= 0.81707\n",
      "Iter 247640, Minibatch Loss= 0.402201, Training Accuracy= 0.82927\n",
      "Iter 249280, Minibatch Loss= 0.487869, Training Accuracy= 0.74390\n",
      "Iter 250920, Minibatch Loss= 0.394281, Training Accuracy= 0.82317\n",
      "Iter 252560, Minibatch Loss= 0.453635, Training Accuracy= 0.79268\n",
      "Iter 254200, Minibatch Loss= 0.375232, Training Accuracy= 0.83537\n",
      "Iter 255840, Minibatch Loss= 0.507219, Training Accuracy= 0.76220\n",
      "Iter 257480, Minibatch Loss= 0.417900, Training Accuracy= 0.82317\n",
      "Iter 259120, Minibatch Loss= 0.401459, Training Accuracy= 0.81098\n",
      "Iter 260760, Minibatch Loss= 0.421646, Training Accuracy= 0.79878\n",
      "Iter 262400, Minibatch Loss= 0.416323, Training Accuracy= 0.82317\n",
      "Iter 264040, Minibatch Loss= 0.379007, Training Accuracy= 0.82317\n",
      "Iter 265680, Minibatch Loss= 0.456834, Training Accuracy= 0.77439\n",
      "Iter 267320, Minibatch Loss= 0.475949, Training Accuracy= 0.77439\n",
      "Iter 268960, Minibatch Loss= 0.409128, Training Accuracy= 0.83537\n",
      "Iter 270600, Minibatch Loss= 0.427541, Training Accuracy= 0.81707\n",
      "Iter 272240, Minibatch Loss= 0.441204, Training Accuracy= 0.78049\n",
      "Iter 273880, Minibatch Loss= 0.455275, Training Accuracy= 0.78049\n",
      "Iter 275520, Minibatch Loss= 0.429691, Training Accuracy= 0.79878\n",
      "Iter 277160, Minibatch Loss= 0.604967, Training Accuracy= 0.68902\n",
      "Iter 278800, Minibatch Loss= 0.412804, Training Accuracy= 0.81098\n",
      "Iter 280440, Minibatch Loss= 0.428258, Training Accuracy= 0.79878\n",
      "Iter 282080, Minibatch Loss= 0.464631, Training Accuracy= 0.82317\n",
      "Iter 283720, Minibatch Loss= 0.390403, Training Accuracy= 0.81707\n",
      "Iter 285360, Minibatch Loss= 0.470433, Training Accuracy= 0.72561\n",
      "Iter 287000, Minibatch Loss= 0.489843, Training Accuracy= 0.77439\n",
      "Iter 288640, Minibatch Loss= 0.432755, Training Accuracy= 0.79268\n",
      "Iter 290280, Minibatch Loss= 0.442969, Training Accuracy= 0.81098\n",
      "Iter 291920, Minibatch Loss= 0.434008, Training Accuracy= 0.80488\n",
      "Iter 293560, Minibatch Loss= 0.451799, Training Accuracy= 0.79878\n",
      "Iter 295200, Minibatch Loss= 0.398691, Training Accuracy= 0.82317\n",
      "Iter 296840, Minibatch Loss= 0.515903, Training Accuracy= 0.74390\n",
      "Iter 298480, Minibatch Loss= 0.388479, Training Accuracy= 0.84756\n",
      "Iter 300120, Minibatch Loss= 0.355380, Training Accuracy= 0.80488\n",
      "Iter 301760, Minibatch Loss= 0.427554, Training Accuracy= 0.80488\n",
      "Iter 303400, Minibatch Loss= 0.380898, Training Accuracy= 0.85366\n",
      "Iter 305040, Minibatch Loss= 0.404128, Training Accuracy= 0.82317\n",
      "Iter 306680, Minibatch Loss= 0.469986, Training Accuracy= 0.75610\n",
      "Iter 308320, Minibatch Loss= 0.378231, Training Accuracy= 0.81098\n",
      "Iter 309960, Minibatch Loss= 0.475269, Training Accuracy= 0.77439\n",
      "Iter 311600, Minibatch Loss= 0.451640, Training Accuracy= 0.78659\n",
      "Iter 313240, Minibatch Loss= 0.400450, Training Accuracy= 0.81098\n",
      "Iter 314880, Minibatch Loss= 0.413358, Training Accuracy= 0.82927\n",
      "Iter 316520, Minibatch Loss= 0.401052, Training Accuracy= 0.79878\n",
      "Iter 318160, Minibatch Loss= 0.425169, Training Accuracy= 0.79878\n",
      "Iter 319800, Minibatch Loss= 0.423596, Training Accuracy= 0.79878\n",
      "Iter 321440, Minibatch Loss= 0.406949, Training Accuracy= 0.81707\n",
      "Iter 323080, Minibatch Loss= 0.371467, Training Accuracy= 0.84756\n",
      "Iter 324720, Minibatch Loss= 0.421361, Training Accuracy= 0.78659\n",
      "Iter 326360, Minibatch Loss= 0.391378, Training Accuracy= 0.84756\n",
      "Iter 328000, Minibatch Loss= 0.393527, Training Accuracy= 0.80488\n",
      "Iter 329640, Minibatch Loss= 0.444983, Training Accuracy= 0.79878\n",
      "Iter 331280, Minibatch Loss= 0.408252, Training Accuracy= 0.82317\n",
      "Iter 332920, Minibatch Loss= 0.464270, Training Accuracy= 0.79268\n",
      "Iter 334560, Minibatch Loss= 0.405291, Training Accuracy= 0.79878\n",
      "Iter 336200, Minibatch Loss= 0.379313, Training Accuracy= 0.84146\n",
      "Iter 337840, Minibatch Loss= 0.409428, Training Accuracy= 0.81707\n",
      "Iter 339480, Minibatch Loss= 0.392678, Training Accuracy= 0.80488\n",
      "Iter 341120, Minibatch Loss= 0.439351, Training Accuracy= 0.78659\n",
      "Iter 342760, Minibatch Loss= 0.414870, Training Accuracy= 0.82927\n",
      "Iter 344400, Minibatch Loss= 0.423949, Training Accuracy= 0.81098\n",
      "Iter 346040, Minibatch Loss= 0.504512, Training Accuracy= 0.76829\n",
      "Iter 347680, Minibatch Loss= 0.561613, Training Accuracy= 0.70732\n",
      "Iter 349320, Minibatch Loss= 0.392383, Training Accuracy= 0.85976\n",
      "Iter 350960, Minibatch Loss= 0.555484, Training Accuracy= 0.72561\n",
      "Iter 352600, Minibatch Loss= 0.423747, Training Accuracy= 0.78049\n",
      "Iter 354240, Minibatch Loss= 0.494826, Training Accuracy= 0.73780\n",
      "Iter 355880, Minibatch Loss= 0.533171, Training Accuracy= 0.69512\n",
      "Iter 357520, Minibatch Loss= 0.434484, Training Accuracy= 0.80488\n",
      "Iter 359160, Minibatch Loss= 0.370891, Training Accuracy= 0.82927\n",
      "Iter 360800, Minibatch Loss= 0.432770, Training Accuracy= 0.78659\n",
      "Iter 362440, Minibatch Loss= 0.417354, Training Accuracy= 0.82927\n",
      "Iter 364080, Minibatch Loss= 0.470820, Training Accuracy= 0.79878\n",
      "Iter 365720, Minibatch Loss= 0.340090, Training Accuracy= 0.86585\n",
      "Iter 367360, Minibatch Loss= 0.322814, Training Accuracy= 0.86585\n",
      "Iter 369000, Minibatch Loss= 0.357777, Training Accuracy= 0.85976\n",
      "Iter 370640, Minibatch Loss= 0.431756, Training Accuracy= 0.79878\n",
      "Iter 372280, Minibatch Loss= 0.418376, Training Accuracy= 0.78049\n",
      "Iter 373920, Minibatch Loss= 0.423338, Training Accuracy= 0.80488\n",
      "Iter 375560, Minibatch Loss= 0.390674, Training Accuracy= 0.85366\n",
      "Iter 377200, Minibatch Loss= 0.378989, Training Accuracy= 0.84146\n",
      "Iter 378840, Minibatch Loss= 0.453453, Training Accuracy= 0.79268\n",
      "Iter 380480, Minibatch Loss= 0.416241, Training Accuracy= 0.78659\n",
      "Iter 382120, Minibatch Loss= 0.429203, Training Accuracy= 0.76829\n",
      "Iter 383760, Minibatch Loss= 0.456883, Training Accuracy= 0.79268\n",
      "Iter 385400, Minibatch Loss= 0.314345, Training Accuracy= 0.87805\n",
      "Iter 387040, Minibatch Loss= 0.388730, Training Accuracy= 0.79878\n",
      "Iter 388680, Minibatch Loss= 0.377898, Training Accuracy= 0.84756\n",
      "Iter 390320, Minibatch Loss= 0.374421, Training Accuracy= 0.84146\n",
      "Iter 391960, Minibatch Loss= 0.308181, Training Accuracy= 0.90244\n",
      "Iter 393600, Minibatch Loss= 0.395085, Training Accuracy= 0.82927\n",
      "Iter 395240, Minibatch Loss= 0.400075, Training Accuracy= 0.83537\n",
      "Iter 396880, Minibatch Loss= 0.361159, Training Accuracy= 0.86585\n",
      "Iter 398520, Minibatch Loss= 0.446870, Training Accuracy= 0.76829\n",
      "Iter 400160, Minibatch Loss= 0.376317, Training Accuracy= 0.86585\n",
      "Iter 401800, Minibatch Loss= 0.372901, Training Accuracy= 0.84756\n",
      "Iter 403440, Minibatch Loss= 0.404027, Training Accuracy= 0.81707\n",
      "Iter 405080, Minibatch Loss= 0.446500, Training Accuracy= 0.82927\n",
      "Iter 406720, Minibatch Loss= 0.358755, Training Accuracy= 0.85976\n",
      "Iter 408360, Minibatch Loss= 0.526737, Training Accuracy= 0.76220\n",
      "Iter 410000, Minibatch Loss= 0.408794, Training Accuracy= 0.83537\n",
      "Iter 411640, Minibatch Loss= 0.456906, Training Accuracy= 0.80488\n",
      "Iter 413280, Minibatch Loss= 0.384092, Training Accuracy= 0.83537\n",
      "Iter 414920, Minibatch Loss= 0.415928, Training Accuracy= 0.80488\n",
      "Iter 416560, Minibatch Loss= 0.376406, Training Accuracy= 0.83537\n",
      "Iter 418200, Minibatch Loss= 0.345344, Training Accuracy= 0.87195\n",
      "Iter 419840, Minibatch Loss= 0.342158, Training Accuracy= 0.85366\n",
      "Iter 421480, Minibatch Loss= 0.383378, Training Accuracy= 0.82927\n",
      "Iter 423120, Minibatch Loss= 0.321473, Training Accuracy= 0.86585\n",
      "Iter 424760, Minibatch Loss= 0.370280, Training Accuracy= 0.83537\n",
      "Iter 426400, Minibatch Loss= 0.454148, Training Accuracy= 0.78659\n",
      "Iter 428040, Minibatch Loss= 0.385514, Training Accuracy= 0.80488\n",
      "Iter 429680, Minibatch Loss= 0.412685, Training Accuracy= 0.82927\n",
      "Iter 431320, Minibatch Loss= 0.380846, Training Accuracy= 0.84146\n",
      "Iter 432960, Minibatch Loss= 0.442021, Training Accuracy= 0.80488\n",
      "Iter 434600, Minibatch Loss= 0.341267, Training Accuracy= 0.87805\n",
      "Iter 436240, Minibatch Loss= 0.464424, Training Accuracy= 0.78049\n",
      "Iter 437880, Minibatch Loss= 0.425810, Training Accuracy= 0.80488\n",
      "Iter 439520, Minibatch Loss= 0.432271, Training Accuracy= 0.82317\n",
      "Iter 441160, Minibatch Loss= 0.364194, Training Accuracy= 0.84756\n",
      "Iter 442800, Minibatch Loss= 0.386011, Training Accuracy= 0.83537\n",
      "Iter 444440, Minibatch Loss= 0.377097, Training Accuracy= 0.81098\n",
      "Iter 446080, Minibatch Loss= 0.420830, Training Accuracy= 0.81098\n",
      "Iter 447720, Minibatch Loss= 0.412184, Training Accuracy= 0.78659\n",
      "Iter 449360, Minibatch Loss= 0.332737, Training Accuracy= 0.86585\n",
      "Iter 451000, Minibatch Loss= 0.319790, Training Accuracy= 0.88415\n",
      "Iter 452640, Minibatch Loss= 0.329968, Training Accuracy= 0.87195\n",
      "Iter 454280, Minibatch Loss= 0.372814, Training Accuracy= 0.85976\n",
      "Iter 455920, Minibatch Loss= 0.367809, Training Accuracy= 0.84146\n",
      "Iter 457560, Minibatch Loss= 0.361853, Training Accuracy= 0.84756\n",
      "Iter 459200, Minibatch Loss= 0.343148, Training Accuracy= 0.85976\n",
      "Iter 460840, Minibatch Loss= 0.374718, Training Accuracy= 0.84146\n",
      "Iter 462480, Minibatch Loss= 0.395455, Training Accuracy= 0.82927\n",
      "Iter 464120, Minibatch Loss= 0.462507, Training Accuracy= 0.78049\n",
      "Iter 465760, Minibatch Loss= 0.393438, Training Accuracy= 0.85366\n",
      "Iter 467400, Minibatch Loss= 0.352762, Training Accuracy= 0.85976\n",
      "Iter 469040, Minibatch Loss= 0.492235, Training Accuracy= 0.75610\n",
      "Iter 470680, Minibatch Loss= 0.436867, Training Accuracy= 0.79268\n",
      "Iter 472320, Minibatch Loss= 0.356590, Training Accuracy= 0.85976\n",
      "Iter 473960, Minibatch Loss= 0.415486, Training Accuracy= 0.81098\n",
      "Iter 475600, Minibatch Loss= 0.309939, Training Accuracy= 0.82927\n",
      "Iter 477240, Minibatch Loss= 0.422634, Training Accuracy= 0.81707\n",
      "Iter 478880, Minibatch Loss= 0.333508, Training Accuracy= 0.85976\n",
      "Iter 480520, Minibatch Loss= 0.332740, Training Accuracy= 0.86585\n",
      "Iter 482160, Minibatch Loss= 0.407509, Training Accuracy= 0.81098\n",
      "Iter 483800, Minibatch Loss= 0.405900, Training Accuracy= 0.79878\n",
      "Iter 485440, Minibatch Loss= 0.427260, Training Accuracy= 0.80488\n",
      "Iter 487080, Minibatch Loss= 0.397548, Training Accuracy= 0.84756\n",
      "Iter 488720, Minibatch Loss= 0.347297, Training Accuracy= 0.82927\n",
      "Iter 490360, Minibatch Loss= 0.300804, Training Accuracy= 0.89024\n",
      "Iter 492000, Minibatch Loss= 0.465115, Training Accuracy= 0.78049\n",
      "Iter 493640, Minibatch Loss= 0.431289, Training Accuracy= 0.81098\n",
      "Iter 495280, Minibatch Loss= 0.425489, Training Accuracy= 0.79878\n",
      "Iter 496920, Minibatch Loss= 0.408061, Training Accuracy= 0.79268\n",
      "Iter 498560, Minibatch Loss= 0.381564, Training Accuracy= 0.83537\n",
      "\n",
      "Testing slicelength: 50, feature_power: 3, learning_rate: 0.001000, n_hidden: 164, batch_size: 164, training_iters: 500000  =  0.823942\n",
      "Iter 1280, Minibatch Loss= 0.932305, Training Accuracy= 0.73438\n",
      "Iter 2560, Minibatch Loss= 0.715053, Training Accuracy= 0.69531\n",
      "Iter 3840, Minibatch Loss= 0.559660, Training Accuracy= 0.69531\n",
      "Iter 5120, Minibatch Loss= 0.604126, Training Accuracy= 0.73438\n",
      "Iter 6400, Minibatch Loss= 0.556884, Training Accuracy= 0.69531\n",
      "Iter 7680, Minibatch Loss= 0.493146, Training Accuracy= 0.77344\n",
      "Iter 8960, Minibatch Loss= 0.503309, Training Accuracy= 0.77344\n",
      "Iter 10240, Minibatch Loss= 0.598041, Training Accuracy= 0.69531\n",
      "Iter 11520, Minibatch Loss= 0.550198, Training Accuracy= 0.73438\n",
      "Iter 12800, Minibatch Loss= 0.591525, Training Accuracy= 0.66406\n",
      "Iter 14080, Minibatch Loss= 0.554376, Training Accuracy= 0.75000\n",
      "Iter 15360, Minibatch Loss= 0.513090, Training Accuracy= 0.74219\n",
      "Iter 16640, Minibatch Loss= 0.516452, Training Accuracy= 0.76562\n",
      "Iter 17920, Minibatch Loss= 0.568297, Training Accuracy= 0.74219\n",
      "Iter 19200, Minibatch Loss= 0.553273, Training Accuracy= 0.71094\n",
      "Iter 20480, Minibatch Loss= 0.556630, Training Accuracy= 0.68750\n",
      "Iter 21760, Minibatch Loss= 0.478853, Training Accuracy= 0.80469\n",
      "Iter 23040, Minibatch Loss= 0.564826, Training Accuracy= 0.72656\n",
      "Iter 24320, Minibatch Loss= 0.512705, Training Accuracy= 0.73438\n",
      "Iter 25600, Minibatch Loss= 0.666604, Training Accuracy= 0.69531\n",
      "Iter 26880, Minibatch Loss= 0.593293, Training Accuracy= 0.71094\n",
      "Iter 28160, Minibatch Loss= 0.525748, Training Accuracy= 0.76562\n",
      "Iter 29440, Minibatch Loss= 0.544530, Training Accuracy= 0.74219\n",
      "Iter 30720, Minibatch Loss= 0.512902, Training Accuracy= 0.75000\n",
      "Iter 32000, Minibatch Loss= 0.443618, Training Accuracy= 0.81250\n",
      "Iter 33280, Minibatch Loss= 0.618483, Training Accuracy= 0.68750\n",
      "Iter 34560, Minibatch Loss= 0.634806, Training Accuracy= 0.65625\n",
      "Iter 35840, Minibatch Loss= 0.497881, Training Accuracy= 0.75000\n",
      "Iter 37120, Minibatch Loss= 0.466923, Training Accuracy= 0.78906\n",
      "Iter 38400, Minibatch Loss= 0.597259, Training Accuracy= 0.68750\n",
      "Iter 39680, Minibatch Loss= 0.522658, Training Accuracy= 0.75000\n",
      "Iter 40960, Minibatch Loss= 0.563183, Training Accuracy= 0.74219\n",
      "Iter 42240, Minibatch Loss= 0.514853, Training Accuracy= 0.70312\n",
      "Iter 43520, Minibatch Loss= 0.510758, Training Accuracy= 0.74219\n",
      "Iter 44800, Minibatch Loss= 0.566609, Training Accuracy= 0.70312\n",
      "Iter 46080, Minibatch Loss= 0.479644, Training Accuracy= 0.79688\n",
      "Iter 47360, Minibatch Loss= 0.600575, Training Accuracy= 0.69531\n",
      "Iter 48640, Minibatch Loss= 0.431867, Training Accuracy= 0.81250\n",
      "Iter 49920, Minibatch Loss= 0.539127, Training Accuracy= 0.76562\n",
      "Iter 51200, Minibatch Loss= 0.515574, Training Accuracy= 0.78125\n",
      "Iter 52480, Minibatch Loss= 0.565901, Training Accuracy= 0.74219\n",
      "Iter 53760, Minibatch Loss= 0.559185, Training Accuracy= 0.71875\n",
      "Iter 55040, Minibatch Loss= 0.586725, Training Accuracy= 0.70312\n",
      "Iter 56320, Minibatch Loss= 0.476036, Training Accuracy= 0.84375\n",
      "Iter 57600, Minibatch Loss= 0.540854, Training Accuracy= 0.71875\n",
      "Iter 58880, Minibatch Loss= 0.526701, Training Accuracy= 0.74219\n",
      "Iter 60160, Minibatch Loss= 0.510192, Training Accuracy= 0.76562\n",
      "Iter 61440, Minibatch Loss= 0.592163, Training Accuracy= 0.67188\n",
      "Iter 62720, Minibatch Loss= 0.565517, Training Accuracy= 0.75000\n",
      "Iter 64000, Minibatch Loss= 0.555324, Training Accuracy= 0.73438\n",
      "Iter 65280, Minibatch Loss= 0.583769, Training Accuracy= 0.71094\n",
      "Iter 66560, Minibatch Loss= 0.554437, Training Accuracy= 0.72656\n",
      "Iter 67840, Minibatch Loss= 0.626231, Training Accuracy= 0.67188\n",
      "Iter 69120, Minibatch Loss= 0.565212, Training Accuracy= 0.73438\n",
      "Iter 70400, Minibatch Loss= 0.566327, Training Accuracy= 0.67188\n",
      "Iter 71680, Minibatch Loss= 0.546376, Training Accuracy= 0.72656\n",
      "Iter 72960, Minibatch Loss= 0.519323, Training Accuracy= 0.80469\n",
      "Iter 74240, Minibatch Loss= 0.556555, Training Accuracy= 0.70312\n",
      "Iter 75520, Minibatch Loss= 0.553889, Training Accuracy= 0.75000\n",
      "Iter 76800, Minibatch Loss= 0.480579, Training Accuracy= 0.79688\n",
      "Iter 78080, Minibatch Loss= 0.463174, Training Accuracy= 0.79688\n",
      "Iter 79360, Minibatch Loss= 0.512568, Training Accuracy= 0.78906\n",
      "Iter 80640, Minibatch Loss= 0.470263, Training Accuracy= 0.76562\n",
      "Iter 81920, Minibatch Loss= 0.399794, Training Accuracy= 0.81250\n",
      "Iter 83200, Minibatch Loss= 0.417157, Training Accuracy= 0.80469\n",
      "Iter 84480, Minibatch Loss= 0.514758, Training Accuracy= 0.75000\n",
      "Iter 85760, Minibatch Loss= 0.437869, Training Accuracy= 0.85156\n",
      "Iter 87040, Minibatch Loss= 0.509644, Training Accuracy= 0.71875\n",
      "Iter 88320, Minibatch Loss= 0.511029, Training Accuracy= 0.71875\n",
      "Iter 89600, Minibatch Loss= 0.565876, Training Accuracy= 0.71094\n",
      "Iter 90880, Minibatch Loss= 0.462730, Training Accuracy= 0.84375\n",
      "Iter 92160, Minibatch Loss= 0.444662, Training Accuracy= 0.85156\n",
      "Iter 93440, Minibatch Loss= 0.485650, Training Accuracy= 0.76562\n",
      "Iter 94720, Minibatch Loss= 0.505406, Training Accuracy= 0.73438\n",
      "Iter 96000, Minibatch Loss= 0.481806, Training Accuracy= 0.78125\n",
      "Iter 97280, Minibatch Loss= 0.561462, Training Accuracy= 0.73438\n",
      "Iter 98560, Minibatch Loss= 0.513353, Training Accuracy= 0.77344\n",
      "Iter 99840, Minibatch Loss= 0.482874, Training Accuracy= 0.81250\n",
      "Iter 101120, Minibatch Loss= 0.491401, Training Accuracy= 0.76562\n",
      "Iter 102400, Minibatch Loss= 0.381447, Training Accuracy= 0.83594\n",
      "Iter 103680, Minibatch Loss= 0.440727, Training Accuracy= 0.83594\n",
      "Iter 104960, Minibatch Loss= 0.425675, Training Accuracy= 0.81250\n",
      "Iter 106240, Minibatch Loss= 0.429070, Training Accuracy= 0.81250\n",
      "Iter 107520, Minibatch Loss= 0.545462, Training Accuracy= 0.75781\n",
      "Iter 108800, Minibatch Loss= 0.444716, Training Accuracy= 0.80469\n",
      "Iter 110080, Minibatch Loss= 0.587618, Training Accuracy= 0.70312\n",
      "Iter 111360, Minibatch Loss= 0.500403, Training Accuracy= 0.77344\n",
      "Iter 112640, Minibatch Loss= 0.426229, Training Accuracy= 0.81250\n",
      "Iter 113920, Minibatch Loss= 0.456609, Training Accuracy= 0.77344\n",
      "Iter 115200, Minibatch Loss= 0.481665, Training Accuracy= 0.78906\n",
      "Iter 116480, Minibatch Loss= 0.468179, Training Accuracy= 0.78125\n",
      "Iter 117760, Minibatch Loss= 0.537372, Training Accuracy= 0.73438\n",
      "Iter 119040, Minibatch Loss= 0.516368, Training Accuracy= 0.74219\n",
      "Iter 120320, Minibatch Loss= 0.489200, Training Accuracy= 0.67188\n",
      "Iter 121600, Minibatch Loss= 0.529729, Training Accuracy= 0.70312\n",
      "Iter 122880, Minibatch Loss= 0.493156, Training Accuracy= 0.79688\n",
      "Iter 124160, Minibatch Loss= 0.457713, Training Accuracy= 0.81250\n",
      "Iter 125440, Minibatch Loss= 0.491334, Training Accuracy= 0.76562\n",
      "Iter 126720, Minibatch Loss= 0.497272, Training Accuracy= 0.78906\n",
      "Iter 128000, Minibatch Loss= 0.485513, Training Accuracy= 0.75781\n",
      "Iter 129280, Minibatch Loss= 0.530181, Training Accuracy= 0.69531\n",
      "Iter 130560, Minibatch Loss= 0.430834, Training Accuracy= 0.82031\n",
      "Iter 131840, Minibatch Loss= 0.457418, Training Accuracy= 0.78906\n",
      "Iter 133120, Minibatch Loss= 0.453735, Training Accuracy= 0.76562\n",
      "Iter 134400, Minibatch Loss= 0.464050, Training Accuracy= 0.79688\n",
      "Iter 135680, Minibatch Loss= 0.491306, Training Accuracy= 0.75000\n",
      "Iter 136960, Minibatch Loss= 0.487052, Training Accuracy= 0.75781\n",
      "Iter 138240, Minibatch Loss= 0.508149, Training Accuracy= 0.77344\n",
      "Iter 139520, Minibatch Loss= 0.540901, Training Accuracy= 0.75000\n",
      "Iter 140800, Minibatch Loss= 0.433289, Training Accuracy= 0.82812\n",
      "Iter 142080, Minibatch Loss= 0.539673, Training Accuracy= 0.73438\n",
      "Iter 143360, Minibatch Loss= 0.535652, Training Accuracy= 0.71875\n",
      "Iter 144640, Minibatch Loss= 0.476630, Training Accuracy= 0.77344\n",
      "Iter 145920, Minibatch Loss= 0.531049, Training Accuracy= 0.73438\n",
      "Iter 147200, Minibatch Loss= 0.429428, Training Accuracy= 0.82031\n",
      "Iter 148480, Minibatch Loss= 0.520018, Training Accuracy= 0.77344\n",
      "Iter 149760, Minibatch Loss= 0.522826, Training Accuracy= 0.72656\n",
      "Iter 151040, Minibatch Loss= 0.487825, Training Accuracy= 0.78125\n",
      "Iter 152320, Minibatch Loss= 0.522485, Training Accuracy= 0.75000\n",
      "Iter 153600, Minibatch Loss= 0.541606, Training Accuracy= 0.77344\n",
      "Iter 154880, Minibatch Loss= 0.497063, Training Accuracy= 0.80469\n",
      "Iter 156160, Minibatch Loss= 0.399924, Training Accuracy= 0.85938\n",
      "Iter 157440, Minibatch Loss= 0.415756, Training Accuracy= 0.83594\n",
      "Iter 158720, Minibatch Loss= 0.382612, Training Accuracy= 0.85156\n",
      "Iter 160000, Minibatch Loss= 0.342386, Training Accuracy= 0.87500\n",
      "Iter 161280, Minibatch Loss= 0.507327, Training Accuracy= 0.76562\n",
      "Iter 162560, Minibatch Loss= 0.495280, Training Accuracy= 0.75000\n",
      "Iter 163840, Minibatch Loss= 0.513814, Training Accuracy= 0.75000\n",
      "Iter 165120, Minibatch Loss= 0.452451, Training Accuracy= 0.84375\n",
      "Iter 166400, Minibatch Loss= 0.412999, Training Accuracy= 0.86719\n",
      "Iter 167680, Minibatch Loss= 0.494122, Training Accuracy= 0.77344\n",
      "Iter 168960, Minibatch Loss= 0.471508, Training Accuracy= 0.77344\n",
      "Iter 170240, Minibatch Loss= 0.530169, Training Accuracy= 0.71094\n",
      "Iter 171520, Minibatch Loss= 0.466659, Training Accuracy= 0.78906\n",
      "Iter 172800, Minibatch Loss= 0.438459, Training Accuracy= 0.80469\n",
      "Iter 174080, Minibatch Loss= 0.463300, Training Accuracy= 0.82031\n",
      "Iter 175360, Minibatch Loss= 0.443617, Training Accuracy= 0.79688\n",
      "Iter 176640, Minibatch Loss= 0.364816, Training Accuracy= 0.82031\n",
      "Iter 177920, Minibatch Loss= 0.470470, Training Accuracy= 0.73438\n",
      "Iter 179200, Minibatch Loss= 0.380121, Training Accuracy= 0.84375\n",
      "Iter 180480, Minibatch Loss= 0.474425, Training Accuracy= 0.82812\n",
      "Iter 181760, Minibatch Loss= 0.504218, Training Accuracy= 0.79688\n",
      "Iter 183040, Minibatch Loss= 0.479608, Training Accuracy= 0.76562\n",
      "Iter 184320, Minibatch Loss= 0.494398, Training Accuracy= 0.79688\n",
      "Iter 185600, Minibatch Loss= 0.530817, Training Accuracy= 0.75000\n",
      "Iter 186880, Minibatch Loss= 0.405351, Training Accuracy= 0.85156\n",
      "Iter 188160, Minibatch Loss= 0.516860, Training Accuracy= 0.83594\n",
      "Iter 189440, Minibatch Loss= 0.513208, Training Accuracy= 0.77344\n",
      "Iter 190720, Minibatch Loss= 0.541004, Training Accuracy= 0.75000\n",
      "Iter 192000, Minibatch Loss= 0.543017, Training Accuracy= 0.75000\n",
      "Iter 193280, Minibatch Loss= 0.579319, Training Accuracy= 0.66406\n",
      "Iter 194560, Minibatch Loss= 0.539138, Training Accuracy= 0.74219\n",
      "Iter 195840, Minibatch Loss= 0.525673, Training Accuracy= 0.76562\n",
      "Iter 197120, Minibatch Loss= 0.569580, Training Accuracy= 0.73438\n",
      "Iter 198400, Minibatch Loss= 0.538823, Training Accuracy= 0.76562\n",
      "Iter 199680, Minibatch Loss= 0.480947, Training Accuracy= 0.71875\n",
      "Iter 200960, Minibatch Loss= 0.497380, Training Accuracy= 0.71094\n",
      "Iter 202240, Minibatch Loss= 0.515109, Training Accuracy= 0.75000\n",
      "Iter 203520, Minibatch Loss= 0.502041, Training Accuracy= 0.72656\n",
      "Iter 204800, Minibatch Loss= 0.417447, Training Accuracy= 0.82812\n",
      "Iter 206080, Minibatch Loss= 0.460012, Training Accuracy= 0.79688\n",
      "Iter 207360, Minibatch Loss= 0.414117, Training Accuracy= 0.79688\n",
      "Iter 208640, Minibatch Loss= 0.438153, Training Accuracy= 0.78906\n",
      "Iter 209920, Minibatch Loss= 0.396827, Training Accuracy= 0.82812\n",
      "Iter 211200, Minibatch Loss= 0.494456, Training Accuracy= 0.76562\n",
      "Iter 212480, Minibatch Loss= 0.485838, Training Accuracy= 0.71094\n",
      "Iter 213760, Minibatch Loss= 0.523080, Training Accuracy= 0.74219\n",
      "Iter 215040, Minibatch Loss= 0.516591, Training Accuracy= 0.71094\n",
      "Iter 216320, Minibatch Loss= 0.377085, Training Accuracy= 0.84375\n",
      "Iter 217600, Minibatch Loss= 0.481440, Training Accuracy= 0.75781\n",
      "Iter 218880, Minibatch Loss= 0.469145, Training Accuracy= 0.81250\n",
      "Iter 220160, Minibatch Loss= 0.416017, Training Accuracy= 0.82031\n",
      "Iter 221440, Minibatch Loss= 0.430262, Training Accuracy= 0.81250\n",
      "Iter 222720, Minibatch Loss= 0.485897, Training Accuracy= 0.75781\n",
      "Iter 224000, Minibatch Loss= 0.567404, Training Accuracy= 0.74219\n",
      "Iter 225280, Minibatch Loss= 0.493025, Training Accuracy= 0.77344\n",
      "Iter 226560, Minibatch Loss= 0.520226, Training Accuracy= 0.76562\n",
      "Iter 227840, Minibatch Loss= 0.507180, Training Accuracy= 0.78906\n",
      "Iter 229120, Minibatch Loss= 0.517653, Training Accuracy= 0.73438\n",
      "Iter 230400, Minibatch Loss= 0.436794, Training Accuracy= 0.79688\n",
      "Iter 231680, Minibatch Loss= 0.452412, Training Accuracy= 0.80469\n",
      "Iter 232960, Minibatch Loss= 0.402181, Training Accuracy= 0.85938\n",
      "Iter 234240, Minibatch Loss= 0.387141, Training Accuracy= 0.85156\n",
      "Iter 235520, Minibatch Loss= 0.534965, Training Accuracy= 0.74219\n",
      "Iter 236800, Minibatch Loss= 0.494561, Training Accuracy= 0.71875\n",
      "Iter 238080, Minibatch Loss= 0.559944, Training Accuracy= 0.73438\n",
      "Iter 239360, Minibatch Loss= 0.496871, Training Accuracy= 0.77344\n",
      "Iter 240640, Minibatch Loss= 0.353432, Training Accuracy= 0.85156\n",
      "Iter 241920, Minibatch Loss= 0.421654, Training Accuracy= 0.80469\n",
      "Iter 243200, Minibatch Loss= 0.388988, Training Accuracy= 0.85156\n",
      "Iter 244480, Minibatch Loss= 0.477247, Training Accuracy= 0.78125\n",
      "Iter 245760, Minibatch Loss= 0.414622, Training Accuracy= 0.80469\n",
      "Iter 247040, Minibatch Loss= 0.416467, Training Accuracy= 0.79688\n",
      "Iter 248320, Minibatch Loss= 0.491457, Training Accuracy= 0.75781\n",
      "Iter 249600, Minibatch Loss= 0.385338, Training Accuracy= 0.84375\n",
      "Iter 250880, Minibatch Loss= 0.468457, Training Accuracy= 0.78125\n",
      "Iter 252160, Minibatch Loss= 0.468408, Training Accuracy= 0.81250\n",
      "Iter 253440, Minibatch Loss= 0.486032, Training Accuracy= 0.78125\n",
      "Iter 254720, Minibatch Loss= 0.482010, Training Accuracy= 0.78125\n",
      "Iter 256000, Minibatch Loss= 0.357769, Training Accuracy= 0.85938\n",
      "Iter 257280, Minibatch Loss= 0.514133, Training Accuracy= 0.74219\n",
      "Iter 258560, Minibatch Loss= 0.523294, Training Accuracy= 0.76562\n",
      "Iter 259840, Minibatch Loss= 0.515173, Training Accuracy= 0.75781\n",
      "Iter 261120, Minibatch Loss= 0.462441, Training Accuracy= 0.80469\n",
      "Iter 262400, Minibatch Loss= 0.456319, Training Accuracy= 0.78906\n",
      "Iter 263680, Minibatch Loss= 0.377221, Training Accuracy= 0.88281\n",
      "Iter 264960, Minibatch Loss= 0.490065, Training Accuracy= 0.78125\n",
      "Iter 266240, Minibatch Loss= 0.482830, Training Accuracy= 0.77344\n",
      "Iter 267520, Minibatch Loss= 0.433591, Training Accuracy= 0.82031\n",
      "Iter 268800, Minibatch Loss= 0.376021, Training Accuracy= 0.83594\n",
      "Iter 270080, Minibatch Loss= 0.411380, Training Accuracy= 0.77344\n",
      "Iter 271360, Minibatch Loss= 0.387846, Training Accuracy= 0.78906\n",
      "Iter 272640, Minibatch Loss= 0.414301, Training Accuracy= 0.82031\n",
      "Iter 273920, Minibatch Loss= 0.435857, Training Accuracy= 0.82031\n",
      "Iter 275200, Minibatch Loss= 0.480818, Training Accuracy= 0.80469\n",
      "Iter 276480, Minibatch Loss= 0.394182, Training Accuracy= 0.82031\n",
      "Iter 277760, Minibatch Loss= 0.554996, Training Accuracy= 0.67969\n",
      "Iter 279040, Minibatch Loss= 0.453372, Training Accuracy= 0.78125\n",
      "Iter 280320, Minibatch Loss= 0.383503, Training Accuracy= 0.82031\n",
      "Iter 281600, Minibatch Loss= 0.474674, Training Accuracy= 0.71875\n",
      "Iter 282880, Minibatch Loss= 0.390999, Training Accuracy= 0.84375\n",
      "Iter 284160, Minibatch Loss= 0.353492, Training Accuracy= 0.88281\n",
      "Iter 285440, Minibatch Loss= 0.387425, Training Accuracy= 0.85156\n",
      "Iter 286720, Minibatch Loss= 0.365862, Training Accuracy= 0.86719\n",
      "Iter 288000, Minibatch Loss= 0.475549, Training Accuracy= 0.78906\n",
      "Iter 289280, Minibatch Loss= 0.453646, Training Accuracy= 0.82031\n",
      "Iter 290560, Minibatch Loss= 0.385691, Training Accuracy= 0.85938\n",
      "Iter 291840, Minibatch Loss= 0.394003, Training Accuracy= 0.82031\n",
      "Iter 293120, Minibatch Loss= 0.398886, Training Accuracy= 0.84375\n",
      "Iter 294400, Minibatch Loss= 0.368627, Training Accuracy= 0.84375\n",
      "Iter 295680, Minibatch Loss= 0.436377, Training Accuracy= 0.78906\n",
      "Iter 296960, Minibatch Loss= 0.441193, Training Accuracy= 0.80469\n",
      "Iter 298240, Minibatch Loss= 0.439164, Training Accuracy= 0.79688\n",
      "Iter 299520, Minibatch Loss= 0.361854, Training Accuracy= 0.85938\n",
      "Iter 300800, Minibatch Loss= 0.450617, Training Accuracy= 0.81250\n",
      "Iter 302080, Minibatch Loss= 0.466890, Training Accuracy= 0.78125\n",
      "Iter 303360, Minibatch Loss= 0.392330, Training Accuracy= 0.82812\n",
      "Iter 304640, Minibatch Loss= 0.484646, Training Accuracy= 0.75781\n",
      "Iter 305920, Minibatch Loss= 0.460934, Training Accuracy= 0.77344\n",
      "Iter 307200, Minibatch Loss= 0.401279, Training Accuracy= 0.85938\n",
      "Iter 308480, Minibatch Loss= 0.431229, Training Accuracy= 0.78906\n",
      "Iter 309760, Minibatch Loss= 0.404425, Training Accuracy= 0.77344\n",
      "Iter 311040, Minibatch Loss= 0.385473, Training Accuracy= 0.82031\n",
      "Iter 312320, Minibatch Loss= 0.414976, Training Accuracy= 0.82812\n",
      "Iter 313600, Minibatch Loss= 0.428912, Training Accuracy= 0.78125\n",
      "Iter 314880, Minibatch Loss= 0.404524, Training Accuracy= 0.82812\n",
      "Iter 316160, Minibatch Loss= 0.388056, Training Accuracy= 0.82812\n",
      "Iter 317440, Minibatch Loss= 0.387025, Training Accuracy= 0.86719\n",
      "Iter 318720, Minibatch Loss= 0.468180, Training Accuracy= 0.74219\n",
      "Iter 320000, Minibatch Loss= 0.467858, Training Accuracy= 0.81250\n",
      "Iter 321280, Minibatch Loss= 0.495353, Training Accuracy= 0.79688\n",
      "Iter 322560, Minibatch Loss= 0.434670, Training Accuracy= 0.82812\n",
      "Iter 323840, Minibatch Loss= 0.458419, Training Accuracy= 0.80469\n",
      "Iter 325120, Minibatch Loss= 0.424004, Training Accuracy= 0.78906\n",
      "Iter 326400, Minibatch Loss= 0.390937, Training Accuracy= 0.82031\n",
      "Iter 327680, Minibatch Loss= 0.409527, Training Accuracy= 0.82031\n",
      "Iter 328960, Minibatch Loss= 0.411788, Training Accuracy= 0.82031\n",
      "Iter 330240, Minibatch Loss= 0.362445, Training Accuracy= 0.89062\n",
      "Iter 331520, Minibatch Loss= 0.449356, Training Accuracy= 0.79688\n",
      "Iter 332800, Minibatch Loss= 0.438750, Training Accuracy= 0.78906\n",
      "Iter 334080, Minibatch Loss= 0.510419, Training Accuracy= 0.71875\n",
      "Iter 335360, Minibatch Loss= 0.483076, Training Accuracy= 0.78125\n",
      "Iter 336640, Minibatch Loss= 0.401374, Training Accuracy= 0.85156\n",
      "Iter 337920, Minibatch Loss= 0.424503, Training Accuracy= 0.80469\n",
      "Iter 339200, Minibatch Loss= 0.382522, Training Accuracy= 0.83594\n",
      "Iter 340480, Minibatch Loss= 0.418693, Training Accuracy= 0.83594\n",
      "Iter 341760, Minibatch Loss= 0.463138, Training Accuracy= 0.82812\n",
      "Iter 343040, Minibatch Loss= 0.335350, Training Accuracy= 0.85938\n",
      "Iter 344320, Minibatch Loss= 0.372635, Training Accuracy= 0.82031\n",
      "Iter 345600, Minibatch Loss= 0.395071, Training Accuracy= 0.80469\n",
      "Iter 346880, Minibatch Loss= 0.465723, Training Accuracy= 0.75781\n",
      "Iter 348160, Minibatch Loss= 0.491940, Training Accuracy= 0.74219\n",
      "Iter 349440, Minibatch Loss= 0.457416, Training Accuracy= 0.81250\n",
      "Iter 350720, Minibatch Loss= 0.494626, Training Accuracy= 0.75781\n",
      "Iter 352000, Minibatch Loss= 0.372594, Training Accuracy= 0.84375\n",
      "Iter 353280, Minibatch Loss= 0.357967, Training Accuracy= 0.83594\n",
      "Iter 354560, Minibatch Loss= 0.383660, Training Accuracy= 0.79688\n",
      "Iter 355840, Minibatch Loss= 0.410568, Training Accuracy= 0.84375\n",
      "Iter 357120, Minibatch Loss= 0.337470, Training Accuracy= 0.85938\n",
      "Iter 358400, Minibatch Loss= 0.324575, Training Accuracy= 0.89844\n",
      "Iter 359680, Minibatch Loss= 0.425747, Training Accuracy= 0.80469\n",
      "Iter 360960, Minibatch Loss= 0.334468, Training Accuracy= 0.89844\n",
      "Iter 362240, Minibatch Loss= 0.414121, Training Accuracy= 0.82031\n",
      "Iter 363520, Minibatch Loss= 0.379175, Training Accuracy= 0.85156\n",
      "Iter 364800, Minibatch Loss= 0.482787, Training Accuracy= 0.78125\n",
      "Iter 366080, Minibatch Loss= 0.320901, Training Accuracy= 0.83594\n",
      "Iter 367360, Minibatch Loss= 0.353826, Training Accuracy= 0.86719\n",
      "Iter 368640, Minibatch Loss= 0.380879, Training Accuracy= 0.83594\n",
      "Iter 369920, Minibatch Loss= 0.421745, Training Accuracy= 0.72656\n",
      "Iter 371200, Minibatch Loss= 0.375430, Training Accuracy= 0.82031\n",
      "Iter 372480, Minibatch Loss= 0.423848, Training Accuracy= 0.84375\n",
      "Iter 373760, Minibatch Loss= 0.394102, Training Accuracy= 0.85156\n",
      "Iter 375040, Minibatch Loss= 0.462354, Training Accuracy= 0.78906\n",
      "Iter 376320, Minibatch Loss= 0.421314, Training Accuracy= 0.77344\n",
      "Iter 377600, Minibatch Loss= 0.333291, Training Accuracy= 0.86719\n",
      "Iter 378880, Minibatch Loss= 0.488082, Training Accuracy= 0.77344\n",
      "Iter 380160, Minibatch Loss= 0.457107, Training Accuracy= 0.80469\n",
      "Iter 381440, Minibatch Loss= 0.402760, Training Accuracy= 0.84375\n",
      "Iter 382720, Minibatch Loss= 0.474019, Training Accuracy= 0.76562\n",
      "Iter 384000, Minibatch Loss= 0.459362, Training Accuracy= 0.77344\n",
      "Iter 385280, Minibatch Loss= 0.479874, Training Accuracy= 0.76562\n",
      "Iter 386560, Minibatch Loss= 0.454930, Training Accuracy= 0.81250\n",
      "Iter 387840, Minibatch Loss= 0.368317, Training Accuracy= 0.82812\n",
      "Iter 389120, Minibatch Loss= 0.389377, Training Accuracy= 0.81250\n",
      "Iter 390400, Minibatch Loss= 0.353836, Training Accuracy= 0.83594\n",
      "Iter 391680, Minibatch Loss= 0.377694, Training Accuracy= 0.85156\n",
      "Iter 392960, Minibatch Loss= 0.494512, Training Accuracy= 0.79688\n",
      "Iter 394240, Minibatch Loss= 0.460814, Training Accuracy= 0.76562\n",
      "Iter 395520, Minibatch Loss= 0.439726, Training Accuracy= 0.82812\n",
      "Iter 396800, Minibatch Loss= 0.313461, Training Accuracy= 0.89062\n",
      "Iter 398080, Minibatch Loss= 0.421837, Training Accuracy= 0.76562\n",
      "Iter 399360, Minibatch Loss= 0.329143, Training Accuracy= 0.85938\n",
      "Iter 400640, Minibatch Loss= 0.320722, Training Accuracy= 0.85156\n",
      "Iter 401920, Minibatch Loss= 0.375866, Training Accuracy= 0.84375\n",
      "Iter 403200, Minibatch Loss= 0.423959, Training Accuracy= 0.80469\n",
      "Iter 404480, Minibatch Loss= 0.436834, Training Accuracy= 0.78125\n",
      "Iter 405760, Minibatch Loss= 0.458846, Training Accuracy= 0.80469\n",
      "Iter 407040, Minibatch Loss= 0.481394, Training Accuracy= 0.73438\n",
      "Iter 408320, Minibatch Loss= 0.464997, Training Accuracy= 0.75000\n",
      "Iter 409600, Minibatch Loss= 0.407152, Training Accuracy= 0.82812\n",
      "Iter 410880, Minibatch Loss= 0.402317, Training Accuracy= 0.79688\n",
      "Iter 412160, Minibatch Loss= 0.437822, Training Accuracy= 0.79688\n",
      "Iter 413440, Minibatch Loss= 0.321702, Training Accuracy= 0.84375\n",
      "Iter 414720, Minibatch Loss= 0.429886, Training Accuracy= 0.82031\n",
      "Iter 416000, Minibatch Loss= 0.468098, Training Accuracy= 0.80469\n",
      "Iter 417280, Minibatch Loss= 0.415336, Training Accuracy= 0.79688\n",
      "Iter 418560, Minibatch Loss= 0.420273, Training Accuracy= 0.80469\n",
      "Iter 419840, Minibatch Loss= 0.415929, Training Accuracy= 0.82812\n",
      "Iter 421120, Minibatch Loss= 0.471079, Training Accuracy= 0.75000\n",
      "Iter 422400, Minibatch Loss= 0.524701, Training Accuracy= 0.64062\n",
      "Iter 423680, Minibatch Loss= 0.562797, Training Accuracy= 0.70312\n",
      "Iter 424960, Minibatch Loss= 0.433121, Training Accuracy= 0.78906\n",
      "Iter 426240, Minibatch Loss= 0.378230, Training Accuracy= 0.83594\n",
      "Iter 427520, Minibatch Loss= 0.512067, Training Accuracy= 0.72656\n",
      "Iter 428800, Minibatch Loss= 0.432993, Training Accuracy= 0.82812\n",
      "Iter 430080, Minibatch Loss= 0.450568, Training Accuracy= 0.77344\n",
      "Iter 431360, Minibatch Loss= 0.455368, Training Accuracy= 0.78125\n",
      "Iter 432640, Minibatch Loss= 0.456755, Training Accuracy= 0.78125\n",
      "Iter 433920, Minibatch Loss= 0.435336, Training Accuracy= 0.77344\n",
      "Iter 435200, Minibatch Loss= 0.498739, Training Accuracy= 0.75000\n",
      "Iter 436480, Minibatch Loss= 0.497146, Training Accuracy= 0.78125\n",
      "Iter 437760, Minibatch Loss= 0.451813, Training Accuracy= 0.78906\n",
      "Iter 439040, Minibatch Loss= 0.566674, Training Accuracy= 0.67188\n",
      "Iter 440320, Minibatch Loss= 0.405142, Training Accuracy= 0.83594\n",
      "Iter 441600, Minibatch Loss= 0.404174, Training Accuracy= 0.78906\n",
      "Iter 442880, Minibatch Loss= 0.413495, Training Accuracy= 0.80469\n",
      "Iter 444160, Minibatch Loss= 0.344873, Training Accuracy= 0.87500\n",
      "Iter 445440, Minibatch Loss= 0.501405, Training Accuracy= 0.74219\n",
      "Iter 446720, Minibatch Loss= 0.439468, Training Accuracy= 0.82031\n",
      "Iter 448000, Minibatch Loss= 0.425933, Training Accuracy= 0.82812\n",
      "Iter 449280, Minibatch Loss= 0.443402, Training Accuracy= 0.78125\n",
      "Iter 450560, Minibatch Loss= 0.456816, Training Accuracy= 0.77344\n",
      "Iter 451840, Minibatch Loss= 0.408031, Training Accuracy= 0.82031\n",
      "Iter 453120, Minibatch Loss= 0.447295, Training Accuracy= 0.82812\n",
      "Iter 454400, Minibatch Loss= 0.451870, Training Accuracy= 0.78906\n",
      "Iter 455680, Minibatch Loss= 0.402718, Training Accuracy= 0.82812\n",
      "Iter 456960, Minibatch Loss= 0.477283, Training Accuracy= 0.73438\n",
      "Iter 458240, Minibatch Loss= 0.554773, Training Accuracy= 0.71094\n",
      "Iter 459520, Minibatch Loss= 0.509280, Training Accuracy= 0.75000\n",
      "Iter 460800, Minibatch Loss= 0.396261, Training Accuracy= 0.85156\n",
      "Iter 462080, Minibatch Loss= 0.430489, Training Accuracy= 0.81250\n",
      "Iter 463360, Minibatch Loss= 0.416374, Training Accuracy= 0.80469\n",
      "Iter 464640, Minibatch Loss= 0.399175, Training Accuracy= 0.80469\n",
      "Iter 465920, Minibatch Loss= 0.353941, Training Accuracy= 0.84375\n",
      "Iter 467200, Minibatch Loss= 0.515429, Training Accuracy= 0.72656\n",
      "Iter 468480, Minibatch Loss= 0.507412, Training Accuracy= 0.76562\n",
      "Iter 469760, Minibatch Loss= 0.412018, Training Accuracy= 0.82031\n",
      "Iter 471040, Minibatch Loss= 0.378972, Training Accuracy= 0.82812\n",
      "Iter 472320, Minibatch Loss= 0.379894, Training Accuracy= 0.85938\n",
      "Iter 473600, Minibatch Loss= 0.384533, Training Accuracy= 0.86719\n",
      "Iter 474880, Minibatch Loss= 0.352155, Training Accuracy= 0.83594\n",
      "Iter 476160, Minibatch Loss= 0.430497, Training Accuracy= 0.77344\n",
      "Iter 477440, Minibatch Loss= 0.527839, Training Accuracy= 0.73438\n",
      "Iter 478720, Minibatch Loss= 0.458952, Training Accuracy= 0.77344\n",
      "Iter 480000, Minibatch Loss= 0.502857, Training Accuracy= 0.78906\n",
      "Iter 481280, Minibatch Loss= 0.459044, Training Accuracy= 0.77344\n",
      "Iter 482560, Minibatch Loss= 0.579482, Training Accuracy= 0.74219\n",
      "Iter 483840, Minibatch Loss= 0.525702, Training Accuracy= 0.76562\n",
      "Iter 485120, Minibatch Loss= 0.409045, Training Accuracy= 0.84375\n",
      "Iter 486400, Minibatch Loss= 0.432726, Training Accuracy= 0.79688\n",
      "Iter 487680, Minibatch Loss= 0.382298, Training Accuracy= 0.84375\n",
      "Iter 488960, Minibatch Loss= 0.441197, Training Accuracy= 0.78906\n",
      "Iter 490240, Minibatch Loss= 0.461712, Training Accuracy= 0.78125\n",
      "Iter 491520, Minibatch Loss= 0.406939, Training Accuracy= 0.79688\n",
      "Iter 492800, Minibatch Loss= 0.408367, Training Accuracy= 0.82031\n",
      "Iter 494080, Minibatch Loss= 0.388141, Training Accuracy= 0.82812\n",
      "Iter 495360, Minibatch Loss= 0.433460, Training Accuracy= 0.82812\n",
      "Iter 496640, Minibatch Loss= 0.351644, Training Accuracy= 0.84375\n",
      "Iter 497920, Minibatch Loss= 0.443658, Training Accuracy= 0.79688\n",
      "Iter 499200, Minibatch Loss= 0.652384, Training Accuracy= 0.71094\n",
      "\n",
      "Testing slicelength: 50, feature_power: 3, learning_rate: 0.001000, n_hidden: 200, batch_size: 128, training_iters: 500000  =  0.778242\n"
     ]
    }
   ],
   "source": [
    "slicelength = 50\n",
    "feature_power = 3\n",
    "learning_rate = 0.001\n",
    "training_iters = 500000\n",
    "test_variables = [\n",
    "    [164, 164],\n",
    "    [200, 128]\n",
    "]\n",
    "\n",
    "data_proportion = 1\n",
    "\n",
    "classical_names = [f for f in os.listdir(\"./classical_short_%i/\" % slicelength)]\n",
    "country_names = [f for f in os.listdir(\"./country_short_%i/\" % slicelength)]\n",
    "\n",
    "num_datapoints = int(min(len(classical_names), len(country_names)) * data_proportion)\n",
    "\n",
    "random.shuffle(classical_names)\n",
    "random.shuffle(country_names)\n",
    "classical_names = classical_names[0:num_datapoints]\n",
    "country_names = country_names[0:num_datapoints]\n",
    "\n",
    "def stft_array(names, kind):\n",
    "    array = []\n",
    "    update_per = len(names)/10\n",
    "    for i, x in enumerate(names):\n",
    "        song_path = '%s_short_%i/%s' % (kind, slicelength, x)\n",
    "        if i%update_per == 0:\n",
    "            sys.stdout.write(str(i/update_per) + \" \")\n",
    "        csong_stft = librosa.stft(librosa.load(song_path)[0])\n",
    "        csong_log_stft = librosa.logamplitude(np.abs(csong_stft)**feature_power,ref_power=np.max)\n",
    "        csong_log_stft = np.array([[abs(item)/80.0 for item in sublist] for sublist in csong_log_stft]) #TODO: consider removing abs / 80\n",
    "        array.append(csong_log_stft.T)\n",
    "    return array\n",
    "\n",
    "#shape is 4 by 1025\n",
    "classical_train = stft_array(classical_names, 'classical')\n",
    "country_train = stft_array(country_names, 'country')\n",
    "\n",
    "test_classical = []\n",
    "test_country = []\n",
    "\n",
    "while len(test_classical) < len(classical_train) / 10:\n",
    "    r = randint(0, len(classical_train) - 1)\n",
    "    test_classical.append(classical_train[r])\n",
    "    del classical_train[r]\n",
    "\n",
    "while len(test_country) < len(country_train) / 10:\n",
    "    r = randint(0, len(country_train) - 1)\n",
    "    test_country.append(country_train[r])\n",
    "    del country_train[r]\n",
    "\n",
    "training = (numpy.array(classical_train + country_train)).astype(numpy.float32)\n",
    "testing = (numpy.array(test_classical + test_country)).astype(numpy.float32)\n",
    "classical_one_hot = numpy.array([0,1])\n",
    "country_one_hot = numpy.array([1,0])\n",
    "training_validation_one_hot = (numpy.array([classical_one_hot for x in classical_train] + [country_one_hot for x in country_train])).astype(numpy.float32)\n",
    "testing_validation_one_hot = (numpy.array([classical_one_hot for x in test_classical] + [country_one_hot for x in test_country])).astype(numpy.float32)\n",
    "\n",
    "def shuffle_array(xs, ys):\n",
    "    zipped = zip(xs,ys)\n",
    "    random.shuffle(zipped)\n",
    "    return zip(*zipped)\n",
    "\n",
    "training, training_validation_one_hot = shuffle_array(training, training_validation_one_hot)\n",
    "testing, testing_validation_one_hot = shuffle_array(testing, testing_validation_one_hot)\n",
    "\n",
    "'''\n",
    "To classify images using a reccurent neural network, we consider every image row as a sequence of pixels.\n",
    "Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = len(training[0][0])\n",
    "n_steps = len(training[0])\n",
    "n_classes = 2\n",
    "\n",
    "for test_variable in test_variables:\n",
    "    n_hidden, batch_size = test_variable\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "    istate = tf.placeholder(\"float\", [None, 2*n_hidden]) #state & cell => 2x n_hidden\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Define weights\n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    def RNN(_X, _istate, _weights, _biases):\n",
    "        _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "        _X = tf.reshape(_X, [-1, n_input]) # (n_steps*batch_size, n_input)\n",
    "        _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)\n",
    "        outputs, states = rnn.rnn(lstm_cell, _X, initial_state=_istate)\n",
    "        return tf.matmul(outputs[-1], _weights['out']) + _biases['out']\n",
    "\n",
    "    pred = RNN(x, istate, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) # Softmax loss\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    def next_batch(xs, ys, batch_num):\n",
    "        length = len(xs)\n",
    "        start = (batch_num * batch_size) % length\n",
    "        if start + batch_size > length:\n",
    "            training, training_validation_one_hot = shuffle_array(xs,ys)\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "        end = start + batch_size\n",
    "        return np.array(xs[start:end]), np.array(ys[start:end])\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        step = 1\n",
    "        # Keep training until reach max iterations\n",
    "        while step * batch_size < training_iters:\n",
    "            batch_xs, batch_ys = next_batch(training, training_validation_one_hot, step)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, istate: np.zeros((batch_size, 2*n_hidden))})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, istate: np.zeros((batch_size, 2*n_hidden))})\n",
    "                # Calculate batch loss\n",
    "                loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, istate: np.zeros((batch_size, 2*n_hidden))})\n",
    "                print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "            step += 1\n",
    "        accuracy = sess.run(accuracy, feed_dict={x: testing, y: testing_validation_one_hot, istate: np.zeros((len(testing), 2*n_hidden))})\n",
    "        variable_array.append([accuracy, slicelength, learning_rate, feature_power, n_hidden, batch_size, training_iters])\n",
    "        print \"\\nTesting slicelength: %i, feature_power: %i, learning_rate: %f, n_hidden: %i, batch_size: %i, training_iters: %i  =  %f\" % (slicelength, feature_power, learning_rate, n_hidden, batch_size, training_iters, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.64864862, 200, 0.001, 3, 200, 128, 50000],\n",
       " [0.71621621, 200, 0.001, 3, 128, 164, 50000],\n",
       " [0.72972971, 50, 0.001, 3, 200, 128, 50000],\n",
       " [0.74324322, 200, 0.001, 3, 164, 164, 50000],\n",
       " [0.74324322, 50, 0.001, 1, 164, 128, 50000],\n",
       " [0.75675678, 200, 0.001, 1, 128, 164, 50000],\n",
       " [0.75675678, 50, 0.001, 1, 164, 164, 50000],\n",
       " [0.77027029, 200, 0.001, 3, 164, 128, 50000],\n",
       " [0.77027029, 50, 0.001, 1, 128, 164, 50000],\n",
       " [0.77027029, 50, 0.001, 3, 128, 128, 50000],\n",
       " [0.77027029, 50, 0.001, 3, 128, 164, 50000],\n",
       " [0.78378379, 200, 0.001, 3, 128, 128, 50000],\n",
       " [0.78378379, 50, 0.001, 3, 200, 164, 50000],\n",
       " [0.7972973, 200, 0.001, 1, 200, 128, 50000],\n",
       " [0.7972973, 50, 0.001, 1, 200, 164, 50000],\n",
       " [0.7972973, 50, 0.001, 3, 164, 128, 50000],\n",
       " [0.8108108, 50, 0.001, 1, 128, 128, 50000],\n",
       " [0.8108108, 50, 0.001, 1, 200, 128, 50000],\n",
       " [0.82432431, 200, 0.001, 1, 128, 128, 50000],\n",
       " [0.82432431, 200, 0.001, 1, 164, 128, 50000],\n",
       " [0.82432431, 200, 0.001, 3, 200, 164, 50000],\n",
       " [0.82432431, 50, 0.001, 3, 164, 164, 50000],\n",
       " [0.86486489, 200, 0.001, 1, 164, 164, 50000],\n",
       " [0.86486489, 200, 0.001, 1, 200, 164, 50000]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "sorted(variable_array, key=itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# To classify images using a reccurent neural network, we consider every image row as a sequence of pixels.\n",
    "# Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 steps for every sample.\n",
    "# '''\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# # Parameters\n",
    "# learning_rate = 0.001\n",
    "# training_iters = 100000\n",
    "# batch_size = 128\n",
    "# display_step = 10\n",
    "\n",
    "# if testing_with_mnist:\n",
    "#     # Network Parameters\n",
    "#     n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "#     n_steps = 28 # timesteps\n",
    "#     n_hidden = 128 # hidden layer num of features\n",
    "#     n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "# else:\n",
    "#     # Network Parameters\n",
    "#     n_input = len(training[0][0])\n",
    "#     n_steps = len(training[0])\n",
    "#     n_hidden = 128 #TODO: play with\n",
    "#     n_classes = 2\n",
    "    \n",
    "# #REMEMBER: mnist is row x column. my data is column x row. shouldn't actually matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # tf Graph input\n",
    "# x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "# istate = tf.placeholder(\"float\", [None, 2*n_hidden]) #state & cell => 2x n_hidden\n",
    "# y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# # Define weights\n",
    "# weights = {\n",
    "#     'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "# }\n",
    "# biases = {\n",
    "#     'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def RNN(_X, _istate, _weights, _biases):\n",
    "#     # input shape: (batch_size, n_steps, n_input)\n",
    "#     _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "#     # Reshape to prepare input to hidden activation\n",
    "#     _X = tf.reshape(_X, [-1, n_input]) # (n_steps*batch_size, n_input)\n",
    "#     # Linear activation\n",
    "#     _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "\n",
    "#     # Define a lstm cell with tensorflow\n",
    "#     lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "#     # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "#     _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)\n",
    "\n",
    "#     # Get lstm cell output\n",
    "#     outputs, states = rnn.rnn(lstm_cell, _X, initial_state=_istate)\n",
    "\n",
    "#     # Linear activation\n",
    "#     # Get inner loop last output\n",
    "#     return tf.matmul(outputs[-1], _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pred = pred if pred is not None else RNN(x, istate, weights, biases)\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) # Softmax loss\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# # Evaluate model\n",
    "# correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def next_batch(xs, ys, batch_num):\n",
    "#     length = len(xs)\n",
    "#     start = (batch_num * batch_size) % length\n",
    "#     if start + batch_size > length:\n",
    "#         training, training_validation_one_hot = shuffle_array(xs,ys)\n",
    "#         # Start next epoch\n",
    "#         start = 0\n",
    "#     end = start + batch_size\n",
    "#     return np.array(xs[start:end]), np.array(ys[start:end])\n",
    "\n",
    "# # Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "# # Launch the graph\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     step = 1\n",
    "#     # Keep training until reach max iterations\n",
    "#     while step * batch_size < training_iters: #TODO: replace with training_iters\n",
    "#         batch_xs, batch_ys = next_batch(training, training_validation_one_hot, step)\n",
    "#         if testing_with_mnist:\n",
    "#             # Reshape data to get 28 seq of 28 elements\n",
    "#             batch_xs = batch_xs.reshape((batch_size, n_steps, n_input))\n",
    "#         # Fit training using batch data\n",
    "#         sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, istate: np.zeros((batch_size, 2*n_hidden))})\n",
    "#         if step % display_step == 0:\n",
    "#             # Calculate batch accuracy\n",
    "#             acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, istate: np.zeros((batch_size, 2*n_hidden))})\n",
    "#             # Calculate batch loss\n",
    "#             loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, istate: np.zeros((batch_size, 2*n_hidden))})\n",
    "#             print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "#         step += 1\n",
    "#     print \"Optimization Finished!\"\n",
    "#     if testing_with_mnist:\n",
    "#         testing = np.array(testing).reshape((-1, 28, 28))\n",
    "#     print \"Testing:\", sess.run(accuracy, feed_dict={x: testing, y: testing_validation_one_hot, istate: np.zeros((len(testing), 2*n_hidden))})\n",
    "# #     classification = sess.run(tf.argmax(pred,1), {x: testing, y: testing_validation_one_hot, istate: np.zeros((len(testing), 2*n_hidden))})\n",
    "# #     print classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
