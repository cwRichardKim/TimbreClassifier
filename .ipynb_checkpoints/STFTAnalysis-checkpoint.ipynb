{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy, scipy, matplotlib.pyplot as plt, pandas, librosa\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num datapoints: 4080\n",
      "update per 40\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 update per 40\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 number of classical slices: 4080\n",
      "number of classical training slices: 3709\n",
      "number of classical testing slices: 371\n",
      "number of country slices: 4080\n",
      "number of country training slices: 3709\n",
      "number of country testing slices: 371\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import random\n",
    "\n",
    "slicelength = 50\n",
    "\n",
    "classical_names = [f for f in os.listdir(\"./classical_short_%i/\" % slicelength)]\n",
    "country_names = [f for f in os.listdir(\"./country_short_%i/\" % slicelength)]\n",
    "\n",
    "data_proportion = .1\n",
    "num_datapoints = int(min(len(classical_names), len(country_names)) * data_proportion)\n",
    "print \"num datapoints: \" + str(num_datapoints)\n",
    "random.shuffle(classical_names)\n",
    "random.shuffle(country_names)\n",
    "classical_names = classical_names[0:num_datapoints]\n",
    "country_names = country_names[0:num_datapoints]\n",
    "\n",
    "def stft_array(names, kind):\n",
    "    update_per = len(names)/100\n",
    "    print \"update per %i\" % update_per\n",
    "    array = []\n",
    "    for i, x in enumerate(names):\n",
    "        song_path = '%s_short_%i/%s' % (kind, slicelength, x)\n",
    "        if i%update_per == 0:\n",
    "            sys.stdout.write(str(i/update_per) + \" \")\n",
    "        csong_stft = librosa.stft(librosa.load(song_path)[0])\n",
    "        csong_log_stft = librosa.logamplitude(np.abs(csong_stft)**2,ref_power=np.max)\n",
    "        csong_log_stft = [item for sublist in csong_log_stft for item in sublist]\n",
    "        array.append(csong_log_stft)\n",
    "    return array\n",
    "    \n",
    "classical_train = stft_array(classical_names, 'classical')\n",
    "country_train = stft_array(country_names, 'country')\n",
    "    \n",
    "from random import randint\n",
    "\n",
    "test_classical = []\n",
    "test_country = []\n",
    "\n",
    "print \"number of classical slices: %i\" % len(classical_train)\n",
    "\n",
    "while len(test_classical) < len(classical_train) / 10:\n",
    "    r = randint(0, len(classical_train) - 1)\n",
    "    test_classical.append(classical_train[r])\n",
    "    del classical_train[r]\n",
    "    \n",
    "print \"number of classical training slices: %i\" % len(classical_train)\n",
    "print \"number of classical testing slices: %i\" % len(test_classical)\n",
    "\n",
    "print \"number of country slices: %i\" % len(country_train)\n",
    "while len(test_country) < len(country_train) / 10:\n",
    "    r = randint(0, len(country_train) - 1)\n",
    "    test_country.append(country_train[r])\n",
    "    del country_train[r]\n",
    "    \n",
    "print \"number of country training slices: %i\" % len(country_train)\n",
    "print \"number of country testing slices: %i\" % len(test_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "55000\n",
      "10000\n",
      "10000\n",
      "784\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "testing_with_mnist = True\n",
    "\n",
    "if testing_with_mnist:\n",
    "    # testing to see if mnist will work here\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    training = mnist.train.images\n",
    "    testing = mnist.test.images\n",
    "    training_validation_one_hot = mnist.train.labels\n",
    "    testing_validation_one_hot = mnist.test.labels\n",
    "else:\n",
    "    training = classical_train + country_train\n",
    "    testing = test_classical + test_country\n",
    "\n",
    "    classical_one_hot = [1,0]\n",
    "    country_one_hot = [0,1]\n",
    "    training_validation_one_hot = [classical_one_hot for x in classical_train] + [country_one_hot for x in country_train]\n",
    "    testing_validation_one_hot = [classical_one_hot for x in test_classical] + [country_one_hot for x in test_country]\n",
    "\n",
    "print len(training)\n",
    "print len(training_validation_one_hot)\n",
    "print len(testing)\n",
    "print len(testing_validation_one_hot)\n",
    "\n",
    "print len(training[0])\n",
    "print len(testing[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_array = [max(x) for x in training]\n",
    "# print \"max val: \" + str(max(max_array))\n",
    "# min_array = [min(x) for x in training]\n",
    "# print \"min val: \" + str(min(min_array))\n",
    "# print \"peek: \" + str(training[0][0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def shuffle_array(xs, ys):\n",
    "    zipped = zip(xs,ys)\n",
    "    random.shuffle(zipped)\n",
    "    return zip(*zipped)\n",
    "print training_validation_one_hot[0:10]\n",
    "training, training_validation_one_hot = shuffle_array(training, training_validation_one_hot)\n",
    "print training_validation_one_hot[0:10]\n",
    "testing, testing_validation_one_hot = shuffle_array(testing, testing_validation_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "numLabels = len(training_validation_one_hot[0])\n",
    "featureSize = len(training[0])\n",
    "print numLabels\n",
    "print featureSize\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, featureSize])\n",
    "W = tf.Variable(tf.zeros([featureSize, numLabels]))\n",
    "b = tf.Variable(tf.zeros([numLabels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, numLabels])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num iters: 5500\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 \n",
      "Training Complete\n",
      "0.8746\n",
      "(10000, 10)\n",
      "(10000, 784)\n",
      "7 7\n",
      "2 2\n",
      "1 1\n",
      "0 0\n",
      "4 4\n",
      "1 1\n",
      "4 4\n",
      "9 9\n",
      "6 5\n",
      "9 9\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "training_length = len(training)\n",
    "batch_size = 10\n",
    "num_iters = training_length / batch_size\n",
    "print \"num iters: \"+str(num_iters)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    if i%(num_iters/100) == 0:\n",
    "        sys.stdout.write(str(i/(num_iters/100)) + \" \")\n",
    "    start = i*batch_size\n",
    "    end = (i+1)*batch_size\n",
    "    batch_xs = training[start:end]\n",
    "    batch_ys = training_validation_one_hot[start:end]\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "print \"\\nTraining Complete\"\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print (sess.run(accuracy, feed_dict={x: testing, y_: testing_validation_one_hot}))\n",
    "testing = numpy.asarray(testing)\n",
    "feed_dict = {x: testing}\n",
    "classification = sess.run(y, feed_dict)\n",
    "print classification.shape\n",
    "print testing.shape\n",
    "for i in range(10):\n",
    "    test = numpy.asarray(classification[i])\n",
    "    label = numpy.asarray(testing_validation_one_hot[i])\n",
    "    print str(test.argmax()) + \" \" + str(label.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.variables.Variable object at 0x14d514350>\n"
     ]
    }
   ],
   "source": [
    "print W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
