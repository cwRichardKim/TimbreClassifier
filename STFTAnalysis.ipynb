{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy, scipy, matplotlib.pyplot as plt, pandas, librosa\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import random\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num datapoints: 408\n",
      "update per 40\n",
      "0 adding classical_short_50/song3_4257.mp3 to classical\n",
      "1 adding classical_short_50/song5_1822.mp3 to classical\n",
      "2 adding classical_short_50/song9_768.mp3 to classical\n",
      "3 adding classical_short_50/song6_2801.mp3 to classical\n",
      "4 adding classical_short_50/song3_1971.mp3 to classical\n",
      "5 adding classical_short_50/song9_3734.mp3 to classical\n",
      "6 adding classical_short_50/song5_3553.mp3 to classical\n",
      "7 adding classical_short_50/song6_4625.mp3 to classical\n",
      "8 adding classical_short_50/song9_4760.mp3 to classical\n",
      "9 adding classical_short_50/song6_5320.mp3 to classical\n",
      "10 adding classical_short_50/song0_307.mp3 to classical\n",
      "update per 40\n",
      "0 adding country_short_50/song8_3280.mp3 to country\n",
      "1 adding country_short_50/song9_2724.mp3 to country\n",
      "2 adding country_short_50/song0_1321.mp3 to country\n",
      "3 adding country_short_50/song9_1466.mp3 to country\n",
      "4 adding country_short_50/song6_611.mp3 to country\n",
      "5 adding country_short_50/song7_2133.mp3 to country\n",
      "6 adding country_short_50/song8_770.mp3 to country\n",
      "7 adding country_short_50/song1_1647.mp3 to country\n",
      "8 adding country_short_50/song5_1101.mp3 to country\n",
      "9 adding country_short_50/song4_2430.mp3 to country\n",
      "10 adding country_short_50/song1_1416.mp3 to country\n",
      "number of classical slices: 408\n",
      "number of classical training slices: 371\n",
      "number of classical testing slices: 37\n",
      "number of country slices: 408\n",
      "number of country training slices: 371\n",
      "number of country testing slices: 37\n"
     ]
    }
   ],
   "source": [
    "slicelength = 50\n",
    "feature_power = 2 #TODO play with this\n",
    "data_proportion = .01\n",
    "\n",
    "classical_names = [f for f in os.listdir(\"./classical_short_%i/\" % slicelength)]\n",
    "country_names = [f for f in os.listdir(\"./country_short_%i/\" % slicelength)]\n",
    "\n",
    "num_datapoints = int(min(len(classical_names), len(country_names)) * data_proportion)\n",
    "print \"num datapoints: \" + str(num_datapoints)\n",
    "\n",
    "random.shuffle(classical_names)\n",
    "random.shuffle(country_names)\n",
    "classical_names = classical_names[0:num_datapoints]\n",
    "country_names = country_names[0:num_datapoints]\n",
    "\n",
    "def stft_array(names, kind):\n",
    "    update_per = len(names)/10\n",
    "    print \"update per %i\" % update_per\n",
    "    array = []\n",
    "    for i, x in enumerate(names):\n",
    "        song_path = '%s_short_%i/%s' % (kind, slicelength, x)\n",
    "        if i%update_per == 0:\n",
    "            sys.stdout.write(str(i/update_per) + \" \")\n",
    "            print \"adding \" + song_path + \" to \" + kind\n",
    "        csong_stft = librosa.stft(librosa.load(song_path)[0])\n",
    "        csong_log_stft = librosa.logamplitude(np.abs(csong_stft)**feature_power,ref_power=np.max)\n",
    "        csong_log_stft = [abs(item)/80.0 for sublist in csong_log_stft for item in sublist] #TODO: consider removing abs / 80\n",
    "        array.append(csong_log_stft)\n",
    "    return array\n",
    "\n",
    "#shape is 4 by 1025\n",
    "classical_train = stft_array(classical_names, 'classical')\n",
    "country_train = stft_array(country_names, 'country')\n",
    "\n",
    "test_classical = []\n",
    "test_country = []\n",
    "\n",
    "print \"number of classical slices: %i\" % len(classical_train)\n",
    "\n",
    "while len(test_classical) < len(classical_train) / 10:\n",
    "    r = randint(0, len(classical_train) - 1)\n",
    "    test_classical.append(classical_train[r])\n",
    "    del classical_train[r]\n",
    "    \n",
    "print \"number of classical training slices: %i\" % len(classical_train)\n",
    "print \"number of classical testing slices: %i\" % len(test_classical)\n",
    "\n",
    "print \"number of country slices: %i\" % len(country_train)\n",
    "while len(test_country) < len(country_train) / 10:\n",
    "    r = randint(0, len(country_train) - 1)\n",
    "    test_country.append(country_train[r])\n",
    "    del country_train[r]\n",
    "    \n",
    "print \"number of country training slices: %i\" % len(country_train)\n",
    "print \"number of country testing slices: %i\" % len(test_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All types check out\n",
      "training: 742\n",
      "testing: 74\n",
      "num features: 4100\n"
     ]
    }
   ],
   "source": [
    "testing_with_mnist = False\n",
    "\n",
    "if testing_with_mnist:\n",
    "    # testing to see if mnist will work here\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "    training = mnist.train.images\n",
    "    testing = mnist.test.images\n",
    "    training_validation_one_hot = mnist.train.labels\n",
    "    testing_validation_one_hot = mnist.test.labels\n",
    "else:\n",
    "    training = (numpy.array(classical_train + country_train)).astype(numpy.float32)\n",
    "    testing = (numpy.array(test_classical + test_country)).astype(numpy.float32)\n",
    "    classical_one_hot = numpy.array([0,1])\n",
    "    country_one_hot = numpy.array([1,0])\n",
    "    training_validation_one_hot = (numpy.array([classical_one_hot for x in classical_train] + [country_one_hot for x in country_train])).astype(numpy.float32)\n",
    "    testing_validation_one_hot = (numpy.array([classical_one_hot for x in test_classical] + [country_one_hot for x in test_country])).astype(numpy.float32)\n",
    "\n",
    "def type_check(array_types, num_types):\n",
    "    error_flag = False\n",
    "    for x in array_types:\n",
    "        if type(x) is numpy.ndarray:\n",
    "            print \"ARRAY TYPE ERROR\"\n",
    "            error_flag = True\n",
    "    for x in num_types:\n",
    "        if type(x) is numpy.float32:\n",
    "            print \"NUM TYPE ERROR\"\n",
    "            error_flag = True \n",
    "    if not error_flag:\n",
    "        print \"All types check out\"\n",
    "        \n",
    "array_types = [type(training), type(training[0]), type(training_validation_one_hot), type(training_validation_one_hot[0]),\n",
    "              type(testing), type(testing[0]), type(testing_validation_one_hot), type(testing_validation_one_hot[0])]\n",
    "num_types = [type(training[0][0]), type(training_validation_one_hot[0][0]), type(testing[0][0]), type(testing_validation_one_hot[0][0])]\n",
    "type_check(array_types, num_types)      \n",
    "\n",
    "if len(training)==len(training_validation_one_hot) and len(testing) == len(testing_validation_one_hot) and len(training[0])==len(testing[0]):\n",
    "    print \"training: \" + str(len(training))\n",
    "    print \"testing: \" + str(len(testing))\n",
    "    print \"num features: \" + str(len(training[0]))\n",
    "else:\n",
    "    print \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_array = [max(x) for x in training]\n",
    "# print \"max val: \" + str(max(max_array))\n",
    "# min_array = [min(x) for x in training]\n",
    "# print \"min val: \" + str(min(min_array))\n",
    "# print \"peek: \" + str(training[10][0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All types check out\n",
      "All types check out\n"
     ]
    }
   ],
   "source": [
    "def shuffle_array(xs, ys):\n",
    "    zipped = zip(xs,ys)\n",
    "    random.shuffle(zipped)\n",
    "    return zip(*zipped)\n",
    "# print training_validation_one_hot[0:10]\n",
    "type_check(array_types, num_types)\n",
    "\n",
    "training, training_validation_one_hot = shuffle_array(training, training_validation_one_hot)\n",
    "testing, testing_validation_one_hot = shuffle_array(testing, testing_validation_one_hot)\n",
    "\n",
    "# print training_validation_one_hot[0:10]\n",
    "type_check(array_types, num_types)\n",
    "\n",
    "# print training[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num labels: 10\n",
      "num features: 784\n"
     ]
    }
   ],
   "source": [
    "numLabels = len(training_validation_one_hot[0])\n",
    "featureSize = len(training[0])\n",
    "print \"num labels: \"+str(numLabels)\n",
    "print \"num features: \" + str(featureSize)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, featureSize])\n",
    "W = tf.Variable(tf.zeros([featureSize, numLabels]))\n",
    "b = tf.Variable(tf.zeros([numLabels]))\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, numLabels])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#convolution and pooling (may not need this)\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#convolution layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#second conv layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#densely connected layer\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#Readout Layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "#Train and Evaluate\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num iters: 1100\n",
      "step 0, training accuracy 0.96\n",
      "step 1, training accuracy 0.98\n",
      "step 2, training accuracy 1\n",
      "step 3, training accuracy 0.98\n",
      "step 4, training accuracy 0.98\n",
      "step 5, training accuracy 0.94\n",
      "step 6, training accuracy 1\n",
      "step 7, training accuracy 0.98\n",
      "step 8, training accuracy 0.96\n",
      "step 9, training accuracy 1\n",
      "\n",
      "Training Complete\n",
      "Running tests\n",
      "test accuracy 0.9754\n"
     ]
    }
   ],
   "source": [
    "training_length = len(training)\n",
    "batch_size = 50\n",
    "num_iters = training_length / batch_size\n",
    "\n",
    "print \"num iters: \" + str(num_iters)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    start = i*batch_size\n",
    "    end = (i+1)*batch_size\n",
    "    batch_xs = training[start:end]\n",
    "    batch_ys = training_validation_one_hot[start:end]\n",
    "    if i%(num_iters/10) == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i/(num_iters/10), train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "    \n",
    "print \"\\nTraining Complete\"\n",
    "print \"Running tests\"\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: testing, y_: testing_validation_one_hot, keep_prob: 1.0}))\n",
    "\n",
    "# testing = numpy.asarray(testing)\n",
    "# feed_dict = {x: testing}\n",
    "# correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# print (sess.run(accuracy, feed_dict={x: testing, y_: testing_validation_one_hot}))\n",
    "\n",
    "# classification = sess.run(y, {x: testing})\n",
    "# print classification[0:10]\n",
    "# print classification.shape\n",
    "# print testing.shape\n",
    "# for i in range(10):\n",
    "#     test = numpy.asarray(classification[i])\n",
    "#     label = numpy.asarray(testing_validation_one_hot[i])\n",
    "#     print str(test.argmax()) + \" \" + str(label.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
